{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAvwGJB9ZLJn"
   },
   "source": [
    "# Data Science Fundamentals 5\n",
    "\n",
    "Basic introduction on how to perform typical machine learning tasks with Python.\n",
    "\n",
    "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
    "Science IT Support, University Of Bern, 2021\n",
    "\n",
    "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
    "\n",
    "# Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVJn0ilgOS8F"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from matplotlib import  pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imageio import imread\n",
    "from time import time as timer\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import umap\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb9uMlBNrEJg"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
    "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
    "    tar = tarfile.open(path, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfprgIx7rI2J"
   },
   "outputs": [],
   "source": [
    "from utils.routines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHfT21bLjv27"
   },
   "source": [
    "# 1. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUus-6PW95xA"
   },
   "source": [
    "## 1. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0sSzs5h95xC"
   },
   "source": [
    "### Theory overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJj9Ry6M95xF"
   },
   "source": [
    "**Objective:** \n",
    "\n",
    "Clustering techniques divide the set of data into group of atoms having common features. Each data point $p$ gets assigned a label $l_p \\in \\{1,..,K\\}$. In this presentation the data points are supposed to have $D$ features, i.e. each data point belongs to $\\mathbf{R}^D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domains:** \n",
    "\n",
    "- `Cluster centers`. Each cluster center belongs to $\\mathbf{R}^D$. It belongs therefore to the same input space of the datacloud. All combined the cluster centers belong to $\\mathbf{R}^{D \\times N_{clust}}$ and can be saved in a matrix of dimension $D \\times N_{clust}$\n",
    "\n",
    "\n",
    "- `Label assignement`. Each point must be assigned to a label. Given an ordering of the points, a cluster \n",
    "assignement can be represented therefore by a sequence $\\{l_1,...,l_{N_{points}}\\}$. Formally a label assigenemt belongs to $\\{0,...,N_{clust}-1\\}^{N_{points}}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods:** \n",
    "- We call $P_k$ the subset of the data set which gets assigned to class $k \\in \\{0,...,N_{clust}-1\\}$ . \n",
    "K-means aims at minimizing the objective function among all possible label assignments and all possible cluster centers $\\{\\mathbf{c}_k\\}_{k=1,..,N_k}$:\n",
    "\n",
    "$$L(\\text{class_assigment}, \\{\\mathbf{c}_k\\}) = \\sum_k L_k (\\text{class_assigment}, \\{\\mathbf{c}_k\\})$$\n",
    "$$L_k= \\sum_{p \\in P_k} d(\\mathbf{x}_p,\\mathbf{c}_{k})^2$$\n",
    "\n",
    "where d is the metric function suited of the problem of interest. Here we use the Euclidean distance as a metric :\n",
    "\n",
    "$$d(\\mathbf{x}_p,\\mathbf{c}_{k})=|\\mathbf{x}_p-\\mathbf{c}_{k}| \\quad \\quad $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed using a simple technique called \"coordinate descent\", in which we fix one coordinate and optimize the other. Than we iterate. To do this we have to answer two questions:\n",
    "\n",
    "QUESTION 1: Given some cluster centers, what is the optimal class assignement ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Assignments_1.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Assignments_2.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER 1: Each point has to be assigned to the closest cluster center.\n",
    "\n",
    "QUESTION 2: Given a fixed label assignment, what are the optimal cluster centers ?\n",
    "\n",
    "ANSWER 2: If we fix a class_assigment, the optimal $\\mathbf{c}_{k}$ are the centroids given by the expression:\n",
    "$$\\mathbf{c}_k=\\frac{1}{N_k}\\sum_{p \\in P_k} \\mathbf{x_p}$$\n",
    "\n",
    "where $N_k$ is the number of points belonging to cluster $k$ ( magnitude of subset $P_k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/iterations.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFU5M14rLvWE"
   },
   "outputs": [],
   "source": [
    "def get_upated_centers(nclust, points, current_class_assigment):     \n",
    "    #code\n",
    "    return centers \n",
    "\n",
    "def get_upated_class_assigment(current_centers, points, nclust):\n",
    "    #code\n",
    "    return class_assigment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEq3Bb7lLvWJ"
   },
   "source": [
    "Such an algorithm finds local minima and may need to be started several times with different initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyJTD8r495xR"
   },
   "source": [
    "**Terminology and output of a K-means computation:**\n",
    "- *Within-cluster variation* : $L_k$ is called within cluster variation. \n",
    "\n",
    "- *Silhouette score*: K-means clustering fixes the number of clusters a priori. Some technique must be chosen to score the different optimal clusterings for different $k$. One technique chooses the best *Silouhette score*. Intuitively, this evaluates the typical distance of points within a same clusters and compares it against the typical distance of points belonging to neighboring but different clusters ( https://en.wikipedia.org/wiki/Silhouette_(clustering) )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOmoafxD95xT"
   },
   "source": [
    "### Sklearn: implementation and usage of K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxacfstD95xW"
   },
   "source": [
    "We start with a 2D example that can be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osqtrCM295xn"
   },
   "source": [
    "First we load the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6X5O32r95xp"
   },
   "outputs": [],
   "source": [
    "points=km_load_th1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BTlSM4W95x0"
   },
   "source": [
    "Explore the data-set checking the dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsW9d6x_95x3"
   },
   "outputs": [],
   "source": [
    "print(points.shape)\n",
    "print('We have ', points.shape[0], 'points with two features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usXcr6XP95yD"
   },
   "outputs": [],
   "source": [
    "plt.plot(points[:,0],points[:,1],'o')\n",
    "plt.xlabel('feature-1')\n",
    "plt.ylabel('feature-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kLezqgz95yQ"
   },
   "source": [
    "It looks visually that the data set has three clusters. We will cluster them using K-means. As usual, we create a KMeans object. Note that we do not need to initialize it with a data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cjLh0ZX95yS"
   },
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHf2oH7p95yf"
   },
   "source": [
    "A call to the fit method computes the cluster centers which can be plotted alongside the data-set. They are accessible from the cluster_centers_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET3C5t-r95yh"
   },
   "outputs": [],
   "source": [
    "clusterer.fit(points)\n",
    "plt.plot(points[:,0],points[:,1],'o')\n",
    "plt.plot(clusterer.cluster_centers_[:,0],clusterer.cluster_centers_[:,1],'o',markersize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4ou8qEVLvWy"
   },
   "outputs": [],
   "source": [
    "clusterer.cluster_centers_[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7H4hw4l95yu"
   },
   "source": [
    "The predict method assigns a new point to the nearest cluster. We can use predict with the training dataset and color the data-set according to the cluster label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqPcVkcR95yw"
   },
   "outputs": [],
   "source": [
    "cluster_labels=clusterer.predict(points)\n",
    "plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBtdsqxK95y6"
   },
   "source": [
    "Finally, we can try to vary the number of clusters and score them with the Silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dg8v1ST95y8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sil=[]\n",
    "\n",
    "for iclust in range(2,6):\n",
    "    clusterer = KMeans(n_clusters=iclust, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(points)\n",
    "    score=silhouette_score(points,cluster_labels)\n",
    "    sil.append(score)\n",
    "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
    "    plt.show()\n",
    "    \n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.plot(np.arange(len(sil))+2, sil,'-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RVDOdZB95zI"
   },
   "source": [
    "The same techniques can be used on high dimensional data-sets. We use here the famous MNIST dataset for integer digits, that we are downloading from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSzh1J9q95zJ"
   },
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "X=train_images[:5000,:].reshape(5000,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTguyfEH95zV"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "image=X[1232,:].reshape(28,28)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08EWsPIR95zh"
   },
   "source": [
    "We can cluster the images exactly as we did for the 2d dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TU70SxP95zm"
   },
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=10, random_state=10)\n",
    "cluster_labels = clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2QNHiFt95zx"
   },
   "source": [
    "We can plot the cluster centers (which are 2D figures!) to see if the clustering is learning correct patterns! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba_LXoum95z0"
   },
   "outputs": [],
   "source": [
    "for iclust in range(10):\n",
    "    plt.imshow(clusterer.cluster_centers_[iclust].reshape(28,28))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTiCQE395z9"
   },
   "source": [
    "You can see that the model looks to assign one class to the same good. Nevertheless, using the cluster centers and with a further trick, in exercise 2 you will build a digit recognition model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jInWo3Uk95z_"
   },
   "source": [
    "### EXERCISE 1: Discover the number of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wua7v8BhvKd3"
   },
   "outputs": [],
   "source": [
    "### In this exercise you are given the dataset points, consisting of high-dimensional data. It was built taking random \n",
    "# samples from a number k of multimensional gaussians. The data is therefore made of k clusters but, being \n",
    "# very high dimensional, you cannot visualize it. Your task it to use K-means combined with the Silouhette \n",
    "# score to find the number of k.\n",
    "\n",
    "# 1. Load the data using the function points=load_ex1_data_clust() , check the dimensionality of the data.\n",
    "\n",
    "# 2. Fix the number of clusters k and define a KMeans clusterer object. Perform the fitting and compute the Silhouette score. \n",
    "# Save the results on a list. \n",
    "\n",
    "# 3. Plot the Silhouette scores as a function ok k? What is the number of clusters ?\n",
    "\n",
    "# 4. Optional. Check the result that you found via umap. Remember the syntax umap_model=umap.UMAP(random_state=xxx) to \n",
    "# istantiate the umap model and than use fit_transform to compute the coordinate with the reduced dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9TSuObo950Z"
   },
   "source": [
    "### EXERCISE 2: Predict the garment using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSyJzomm950e"
   },
   "outputs": [],
   "source": [
    "###### DESCRIPTION ###############\n",
    "\n",
    "# In this exercise you are asked to use the clustering performed by K-means to predict the good in the F-mnist dataset. \n",
    "#\n",
    "# We use clustering as a preprocessing for a supervised task.\n",
    "#\n",
    "# We will follow the pipeline to fit the model :\n",
    "#\n",
    "# 1- We perform K-means clustering using just the input data and fixing for the start the number of clusters to 10 ;\n",
    "# 2- To each cluster, we will attach a label, finding the most represented good inside that cluster. Let's call that label\n",
    "# assignment[c] for cluster c ;  \n",
    "# \n",
    "# When using the model for prediction of a new image we will :\n",
    "#\n",
    "# 1- Find the cluster center nearest to the new image ;\n",
    "# 2- Assign the new image to the good most represented in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hu2_V7mmLvX4"
   },
   "outputs": [],
   "source": [
    "# Follow the following STEPS to solve the exercise\n",
    "\n",
    "# STEP 1. Load the dataset.\n",
    "\n",
    "#fmnist = tf.keras.datasets.fashion_mnist\n",
    "#(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "#X_train=train_images[:5000,:].reshape(5000,-1)\n",
    "#y_train=train_labels[:5000]\n",
    "\n",
    "#X_test=test_images[:1000,:].reshape(1000,-1)\n",
    "#y_test=test_labels[:1000]\n",
    "\n",
    "\n",
    "# STEP 2. \n",
    "# Define the cluster KMeans object and fit the model on the training set. \n",
    "\n",
    "# STEP 3. \n",
    "# Call the predict method of the KMeans object you defined on the training set and compute the cluster labels. \n",
    "# \n",
    "\n",
    "# STEP 4. \n",
    "# Compute the assignment list. assignment[i] will be the majority class of the i-cluster  \n",
    "# You can use, if you want,  the function most_common with arguments (k,y_train, cluster_labels) \n",
    "# this compute the assignment list. \n",
    "\n",
    "def most_common(nclusters, supervised_labels, cluster_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - nclusters : the number of clusters\n",
    "    - supervised_labels : for each garment, the labelling provided by the training data ( e.g. in y_train or y_test)\n",
    "    - cluster_labels : for each garment, the cluster it was assigned by K-Means using the predict method of the Kmeans object\n",
    "    \n",
    "    Returns:\n",
    "    - a list \"assignment\" of lengths nclusters, where assignment[i] is the majority class of the i-cluster \n",
    "    \"\"\"\n",
    "    \n",
    "    assignment=[]\n",
    "    for icluster in range(nclusters):\n",
    "        indices=list(supervised_labels[cluster_labels==icluster])\n",
    "        try:\n",
    "            chosen= max(set(indices), key=indices.count)\n",
    "        except ValueError :\n",
    "            print('Em')\n",
    "            chosen=1\n",
    "        assignment.append(chosen)\n",
    "        \n",
    "    return assignment\n",
    "\n",
    "# STEP 5. \n",
    "# Using the predict method of your KMeans object, predict the cluster labels for the test set using X_test as an argument.\n",
    "\n",
    "# STEP 6.\n",
    "# using the cluster labels predicted in STEP 5 and the previously computed assignment[] list, \n",
    "# predict what are according to your model the predicted goods for the test set, call them new_labels\n",
    "\n",
    "# STEP 6.\n",
    "# Using  a call cm=metrics.confusion_matrix( y_train, new_labels ) you can print the confusion matrix on the test set, which\n",
    "# provides information on the quality of the fit. Print the percentage of correctly classified examples. \n",
    "# For example, you can dividee the sum of the elements on the diagonal of cm and divide by the sum of all entries of cm.  \n",
    "#\n",
    "\n",
    "# STEP 7 (optional).\n",
    "#  Perform again steps 2 / 3 increasing the number of clusters from 10 to 40 what happens to the performance ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd9WGjVQ951B"
   },
   "source": [
    "## 2. Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baYX-IKOB461"
   },
   "source": [
    "### Theory overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkueVtkf951D"
   },
   "source": [
    "K-Means is a modelling procedure which is biased towards clusters of circular shape and therefore does not always work perfectly, as the following examples show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_kqTtgi951F"
   },
   "outputs": [],
   "source": [
    "points=gm_load_th1()\n",
    "clusterer = KMeans(n_clusters=3, random_state=10)\n",
    "cluster_labels=clusterer.fit_predict(points)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title('K-Means')\n",
    "plt.xlim(-6,6)\n",
    "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRsc4ZOV951O"
   },
   "outputs": [],
   "source": [
    "points=gm_load_th2()\n",
    "clusterer = KMeans(n_clusters=2, random_state=10)\n",
    "cluster_labels=clusterer.fit_predict(points)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title('K-Means')\n",
    "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SvVyxXu952E"
   },
   "source": [
    "A Gaussian mixture model is able to fit these kinds of clusters. In a Gaussian mixture model, each data-set is supposed to be a random point from the distribution:\n",
    "$$f(\\mathbf{x})=\\sum_c \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$$\n",
    ", which is called a Gaussian mixture (N stands for Normal distribution). The parameters $\\{\\pi_c,\\mathbf{\\mu_c},\\mathbf{\\Sigma_c}\\}$ are fitted from the data using a minimization procedure (maximum likelihood via the EM algorithm) and $N_c$ is the chosen number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqOLzPso952G"
   },
   "source": [
    "**Output of a GM computation:** \n",
    "- *Cluster probabilities:* A gaussian mixtures model is an example of soft clustering, where each data point $p$ does not get assigned a unique cluster, but a distribution over clusters $f_p(c), c=1,...,N_c$. \n",
    "\n",
    "Given the fitted parameters,  $f_p(c)$ is computed as: $$f_p(c)=\\frac{ \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x_p})}{\\sum_{c'} \\pi_c N(\\mathbf{\\mu_{c'}},\\mathbf{\\Sigma_{c'}} )(\\mathbf{x_p})}, c=1...N_c$$ \n",
    "\n",
    ", where $\\mathbf{x_p}$ are the coordinates of point p. \n",
    "- *AIC/BIC:* after each clustering two numbers are returned. These can be used to select the optimal number of Gaussians to be used, similar to the Silhouette score. ( AIC and BIC consider both the likelihood of the data given the parameters and the complexity of the model related to the number of Gaussians used ). The lowest AIC or BIC value is an indication of a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdddJhgl952H"
   },
   "source": [
    "### Sklearn: implementation and usage of Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCyazHHp952W"
   },
   "source": [
    "First of all, we see how the Gaussian model behaves on our original example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTu1jVUr952Y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points=km_load_th1()\n",
    "\n",
    "aic=[]\n",
    "bic=[]\n",
    "sil=[]\n",
    "\n",
    "for i_comp in range(2,6):\n",
    "    plt.figure()\n",
    "    plt.title(str(i_comp))\n",
    "    clf = GaussianMixture(n_components=i_comp, covariance_type='full')\n",
    "    clf.fit(points)\n",
    "    cluster_labels=clf.predict(points)\n",
    "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
    "    print(i_comp,clf.aic(points),clf.bic(points))\n",
    "    score=silhouette_score(points,cluster_labels)\n",
    "    aic.append(clf.aic(points))\n",
    "    bic.append(clf.bic(points))\n",
    "    sil.append(score)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVTi69ZS952o"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(2,6),aic,'-o')\n",
    "plt.title('aic')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(2,6),bic,'-o')\n",
    "plt.title('bic')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(2,6),sil,'-o')\n",
    "plt.title('silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_RrKQ0W952z"
   },
   "source": [
    "So in this case we get a comparable result, and also the probabilistic tools agree with the Silhouette score ! Let's see how the Gaussian mixtures behave in the examples where K-means was failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKmBGlDr9521"
   },
   "outputs": [],
   "source": [
    "points=gm_load_th1()\n",
    "clf = GaussianMixture(n_components=3, covariance_type='full')\n",
    "clf.fit(points)\n",
    "cluster_labels=clf.predict(points)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title('K-Means')\n",
    "plt.xlim(-6,6)\n",
    "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ubCBvfj952-"
   },
   "outputs": [],
   "source": [
    "points=gm_load_th2()\n",
    "clf = GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(points)\n",
    "cluster_labels=clf.predict(points)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIykj5dp953I"
   },
   "source": [
    "### EXERCISE 3 : Find the prediction uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Hi_F5MhLvYl"
   },
   "outputs": [],
   "source": [
    "###### DESCRIPTION ###############\n",
    "\n",
    "#In this exercise you need to load the dataset used to present K-means ( points=km_load_th1() ) or the one used to discuss \n",
    "# the Gaussian mixtures model ( points=gm_load_th1() ). \n",
    "#\n",
    "# As discussed, applying a fitting based on gaussian mixtures you can not only predict the cluster label for each point, \n",
    "# but also a probability distribution over the clusters. \n",
    "\n",
    "# From this probability distribution, you can compute for each point the entropy of the corresponging \n",
    "# distribution (using for example scipy.stats.entropy) as an estimation of the undertainty of the prediction. \n",
    "# Your task is to plot the data-cloud with a color proportional to the uncertainty of the cluster assignement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBu2TYwN953J"
   },
   "outputs": [],
   "source": [
    "# Follow the following STEPS to solve the exercise\n",
    "\n",
    "# In detail you should:\n",
    "# 0. Load the dataset points=gm_load_th1()\n",
    "# 1. Instantiate a GaussianMixture object with the number of clusters that you expect\n",
    "# 2. fit the object on the dataset with the fit method \n",
    "# 3. compute the cluster probabilities using the method predict_proba. This will return a matrix of dimension \n",
    "# npoints x nclusters\n",
    "# 4. use the entropy function ( from scipy.stats import entropy ) to evaluate for each point the uncertainty of the prediction\n",
    "# 5. Plot the points colored accordingly to their uncertanty. You can use for example the code\n",
    "\n",
    "#cm = plt.cm.get_cmap('RdYlBu')\n",
    "#plt.scatter(x, y, c=colors, cmap=cm)\n",
    "#plt.colorbar(sc)\n",
    "\n",
    "# where `colors` is the list of entropies computed for each point."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dE4fVdZG953V",
    "uHfT21bLjv27",
    "pUus-6PW95xA",
    "T0sSzs5h95xC",
    "gOmoafxD95xT",
    "jInWo3Uk95z_",
    "v9TSuObo950Z",
    "qd9WGjVQ951B",
    "baYX-IKOB461",
    "fdddJhgl952H",
    "IIykj5dp953I",
    "oAAjJuenj1u0",
    "5AXSFimKkt91",
    "C-mH0li3kzNi",
    "8QZtmo6Vk2rp",
    "u62hCoFbklaW",
    "yrwaonuB4F8m",
    "aOk-BLOFeV61"
   ],
   "name": "Course_3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
