{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/neworldemancer/DSF5/blob/master/Course_3_venice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to machine learning & Data Analysis\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Data Science Lab, University Of Bern, 2023\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MohpJtH9effm"
      },
      "outputs": [],
      "source": [
        "!pip install scipy=='1.7.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJn0ilgOS8F"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from matplotlib import  pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread\n",
        "from time import time as timer\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "import umap\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from scipy.stats import entropy\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb9uMlBNrEJg"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfprgIx7rI2J"
      },
      "outputs": [],
      "source": [
        "from utils.routines import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def house_prices_dataset(return_df=False, price_max=400000, area_max=40000):\n",
        "  path = 'data/AmesHousing.csv'\n",
        "\n",
        "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False)\n",
        "\n",
        "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "  useful_fields = ['LotArea',\n",
        "                  'Utilities', 'OverallQual', 'OverallCond',\n",
        "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
        "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
        "                  'FullBath', 'HalfBath',\n",
        "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
        "                  'Functional','PoolArea',\n",
        "                  'YrSold', 'MoSold'\n",
        "                  ]\n",
        "  target_field = 'SalePrice'\n",
        "\n",
        "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
        "\n",
        "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
        "                  'LotFrontage': {'NA':0},\n",
        "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
        "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
        "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
        "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
        "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'CentralAir':  {'N':0, 'Y': 1},\n",
        "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
        "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
        "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
        "                  }\n",
        "\n",
        "  df_X = df[useful_fields].copy()\n",
        "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
        "  df_Y = df[target_field].copy()\n",
        "\n",
        "  x = df_X.to_numpy().astype(np.float32)\n",
        "  y = df_Y.to_numpy().astype(np.float32)\n",
        "\n",
        "  if price_max>0:\n",
        "    idxs = y<price_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  if area_max>0:\n",
        "    idxs = x[:,0]<area_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  return (x, y, df) if return_df else (x,y)\n",
        "\n",
        "\n",
        "def house_prices_dataset_normed():\n",
        "    x, y = house_prices_dataset(return_df=False, price_max=-1, area_max=-1)\n",
        "\n",
        "    scaler=StandardScaler()\n",
        "    features_scaled=scaler.fit_transform(x)\n",
        "\n",
        "    return features_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLUSTERING Objective:** \n",
        "\n",
        "Clustering techniques divide the set of data into group of points having common features. Each data point $p$ gets assigned a label $l_p \\in \\{1,..,K\\}$ representing the group to which it belongs. In this presentation the data points are supposed to have $D$ features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**General Caveat:** \n",
        "\n",
        "Clustering techniques rely on a a-priori fixed notion of 'distance' between the points. This implies:\n",
        "\n",
        "- The default distance is called `euclidean`: $d(x_p,x_{q})=\\sqrt{\\sum_{i} |x_{p,i}-x_{q,i}|^{2}}$. This distance is not invariant if we change units, if all features have difference units. One way to keep on using the default euclidean distance is normalizing variables.\n",
        "\n",
        "- Other ad-hoc distance may be defined depending on the use-case scenario.\n",
        "\n",
        "- Results of clustering must always be checked by experts of the field of interest. Clustering is an iterative process.\n",
        "\n",
        "Note that if your data is labelled (e.g. each label is assigned to a class), the number of groups can be different than the number of classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Non-linear dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Non-linear dimensionality reduction techniques can be used to map the original set of points with $D$ features into 2 features.\n",
        "\n",
        "- PCA also offers such a mapping (take the first two relevant scores) but in general two components are not enough to conserve the variance. This happens because PCA limits itself to new features that are `linear combinations` of the old one\n",
        "\n",
        "- Non-linear dimensionality reduction techniques do not pose themselves such a constraint and therefore can map data into very low dimensions (for example 2)\n",
        "\n",
        "- If we have a map in 2D we can visualize clusters by eye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`t-Sne` and `UMAP` are the two most popular dimensionality reduction techniques. They differ in the type of information that is preserved before and after the mapping.\n",
        "\n",
        "- `t-Sne` uses a `statistical description` of the environment of a sample point ;\n",
        "- `UMAP` conserves the `connectivity` (topological) properties of the environment of a sample point ;\n",
        "\n",
        "The projection on the low-dimensional space is optimized in order to match as much as possible the description of the local environment.\n",
        "\n",
        "It is not the goal of this introduction to discuss the derivation of such approaches, which can be found in the references:\n",
        "\n",
        "https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1802.03426.pdf\n",
        "\n",
        "Results between UMAP and t-Sne shoud be comparable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "n_examples = 5000\n",
        "data=train_images[:n_examples,:].reshape(n_examples,-1)\n",
        "data=data/255\n",
        "\n",
        "labels=train_labels[:n_examples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# not to run on COLAB\n",
        "\n",
        "# tsne_model = TSNE(perplexity=10, n_components=2, learning_rate=200,\n",
        "#                   early_exaggeration=4.0,init='pca',\n",
        "#                   n_iter=2000, random_state=2233212,\n",
        "#                   metric='euclidean', verbose=100, n_jobs=1)\n",
        "\n",
        "# tsne_mnist = tsne_model.fit_transform(data)\n",
        "\n",
        "# plt.scatter(tsne_mnist[:,0],tsne_mnist[:,1],c=labels,s=10)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_mnist.png\" width=\"100%\"/> | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/mnist.png\" width=\"100%\"/> |\n",
        " |  -----:| -----:|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=10, n_components=2, random_state=1711)\n",
        "umap_mnist = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_mnist[:, 0], umap_mnist[:, 1], c=labels, s=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=house_prices_dataset_normed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)\n",
        "umap_houses = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_houses[:, 0], umap_houses[:, 1], s=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUus-6PW95xA"
      },
      "source": [
        "## 2. K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider first that you know the number of groups over which you want to divide your data. For example $K=3$ or $K=10$ groups. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "5XsYvuoUeffq"
      },
      "source": [
        "***-cluster centers***\n",
        "\n",
        "K-means heavily relies on the concept of:\n",
        "\n",
        "`cluster centers`. Each group gets assigned a cluster center, as a mean representative point. The centroids given by the expression:\n",
        "$$\\mathbf{c}_k=\\frac{1}{N_k}\\sum_{p \\in S_k} \\mathbf{x_p}$$\n",
        "\n",
        ", $S_k$ is the subset of points assigned to the same group $k$ and $N_k$ is the number of points in $S_k$.\n",
        "\n",
        "If the data cloud is made of images, it is the image containing at each pixel the mean value of all images of that group. \n",
        "\n",
        "If the data cloud is made of questionnaires, it is the mean value assigned to an answer by all questionnaires belonging to that group.\n",
        "\n",
        "If the data cloud is made of pressure sensors, it assigns to each sensors the mean of the pressure over all time spans belonging to that group, and so on...\n",
        "\n",
        "$K-Means$ assign cluster center to a single group. All points whose closest cluster center belongs to class $K$, is assigned to group $K$ as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wOaFNbi1effq"
      },
      "source": [
        "***-loss function***\n",
        "\n",
        "The cluster centres are chosen in order to minimize the following loss function:\n",
        "\n",
        "$$\\sum_{k \\in \\text{classes}} \\sum_{p \\in S_k} d(\\mathbf{x}_p,\\mathbf{c}_{k})^2$$\n",
        "\n",
        "where $S_k$ is the subset of points assigned to the same group $k$.\n",
        "\n",
        "d is the metric function suited of the problem of interest. Here we use the Euclidean distance as a metric :\n",
        "\n",
        "$$d(\\mathbf{x}_p,\\mathbf{c}_{k})=|\\mathbf{x}_p-\\mathbf{c}_{k}| \\quad \\quad $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2vciYXuTefft"
      },
      "source": [
        "Such an algorithm finds local minima and may need to be started several times with different initializations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "CQTkkdLnefft"
      },
      "source": [
        "**-vary the number of classes and validate the clustering**\n",
        "\n",
        "*Silhouette score*: K-means clustering fixes the number of clusters a priori. Some technique must be chosen to score the different optimal clusterings for different $k$. One technique chooses the best *Silouhette score*. Intuitively, this evaluates the typical distance of points within a same clusters and compares it against the typical distance of points belonging to neighboring but different clusters ( https://en.wikipedia.org/wiki/Silhouette_(clustering) ).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "gOmoafxD95xT"
      },
      "source": [
        "### Sklearn: implementation and usage of K-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bxacfstD95xW"
      },
      "source": [
        "We start with a 2D example that can be visualized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "osqtrCM295xn"
      },
      "source": [
        "First we load the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "C6X5O32r95xp"
      },
      "outputs": [],
      "source": [
        "points=km_load_th1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_BTlSM4W95x0"
      },
      "source": [
        "Explore the data-set checking the dataset dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "LsW9d6x_95x3"
      },
      "outputs": [],
      "source": [
        "print(points.shape)\n",
        "print('We have ', points.shape[0], 'points with two features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "usXcr6XP95yD"
      },
      "outputs": [],
      "source": [
        "plt.plot(points[:,0],points[:,1],'o')\n",
        "plt.xlabel('feature-1')\n",
        "plt.ylabel('feature-2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_kLezqgz95yQ"
      },
      "source": [
        "It looks visually that the data set has three clusters. We will cluster them using K-means. As usual, we create a KMeans object. Note that we do not need to initialize it with a data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "5cjLh0ZX95yS"
      },
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=3, random_state=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "fHf2oH7p95yf"
      },
      "source": [
        "A call to the fit method computes the cluster centers which can be plotted alongside the data-set. They are accessible from the cluster_centers_ attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ET3C5t-r95yh"
      },
      "outputs": [],
      "source": [
        "clusterer.fit(points)\n",
        "plt.plot(points[:,0],points[:,1],'o')\n",
        "plt.plot(clusterer.cluster_centers_[:,0],clusterer.cluster_centers_[:,1],'o',markersize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "z4ou8qEVLvWy"
      },
      "outputs": [],
      "source": [
        "clusterer.cluster_centers_[:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "J7H4hw4l95yu"
      },
      "source": [
        "The predict method assigns a new point to the nearest cluster. We can use predict with the training dataset and color the data-set according to the cluster label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "BqPcVkcR95yw"
      },
      "outputs": [],
      "source": [
        "cluster_labels=clusterer.predict(points)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wBtdsqxK95y6"
      },
      "source": [
        "Finally, we can try to vary the number of clusters and score them with the Silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "_dg8v1ST95y8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sil=[]\n",
        "\n",
        "for iclust in range(2,6):\n",
        "    clusterer = KMeans(n_clusters=iclust, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(points)\n",
        "    score=silhouette_score(points,cluster_labels)\n",
        "    sil.append(score)\n",
        "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "    plt.show()\n",
        "    \n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.plot(np.arange(len(sil))+2, sil,'-o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8RVDOdZB95zI"
      },
      "source": [
        "The same techniques can be used on high dimensional data-sets. We use here the famous MNIST dataset for integer digits, that we are downloading from tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "RSzh1J9q95zJ"
      },
      "outputs": [],
      "source": [
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "X=train_images[:5000,:].reshape(5000,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "FTguyfEH95zV"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "image=X[1232,:].reshape(28,28)\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "08EWsPIR95zh"
      },
      "source": [
        "We can cluster the images exactly as we did for the 2d dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "-TU70SxP95zm"
      },
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=10, random_state=10)\n",
        "cluster_labels = clusterer.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Q2QNHiFt95zx"
      },
      "source": [
        "We can plot the cluster centers (which are 2D figures!) to see if the clustering is learning correct patterns! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ba_LXoum95z0"
      },
      "outputs": [],
      "source": [
        "for iclust in range(10):\n",
        "    plt.imshow(clusterer.cluster_centers_[iclust].reshape(28,28))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1QTiCQE395z9"
      },
      "source": [
        "You can see that the model is trying to assign one class to each group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "b3VoJKsVeff0"
      },
      "source": [
        "## 3. Dendograms (hierarchical clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PfeYW8eKeff0"
      },
      "source": [
        "**Objective:** \n",
        "\n",
        "In hierarchical clustering we do not have only an optimal set of clusters, but for each different \"length scale\" we have a different set of clusters.\n",
        "\n",
        "**Algorithm:** \n",
        "\n",
        "- We start with a length scale $l=0$ at the beginning and consider all sample elements as different clusters.\n",
        "\n",
        "- We increase than $l$ to values larger than zero. Let's call the minimum distance between pair of points $l_0$. As soon as we reach $l=l_0$, these elements are merged into a new cluster (greedy strategy). \n",
        "\n",
        "To proceed further we need to define a distance between subsets $S_1,S_2$ of points. In the \"single linkage\" flavour we define:\n",
        "\n",
        "$$d(S_1,S_2)=min_{a\\in S_1, b\\in S_2} d(a,b)$$\n",
        "\n",
        "- This way we can proceed increasing $l>l_0$. As soon as we find two clusters with distance smaller than $l_1>l_0$, we merge them. \n",
        "\n",
        "- We keep on increasing $l$ as far as one one cluster remains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yDf_mhqdeff0"
      },
      "source": [
        "The result of this clustering procedure can be summarized in a `dendrogram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "xhNhBihUeff0"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from scipy.cluster import hierarchy\n",
        "from ipywidgets import interact\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "points=km_load_th1()\n",
        "Z = hierarchy.linkage(points, 'single')\n",
        "n_clusters=int(np.max(Z[:,[0,1]].flatten()))\n",
        "print(n_clusters)\n",
        "\n",
        "@interact\n",
        "def plot(t=(0,7,0.1)):  \n",
        "    fig, axes=plt.subplots(1,2,figsize=(15,10), gridspec_kw={'width_ratios': [1, 2]})\n",
        "    fl = fcluster(Z,t=t,criterion='distance')\n",
        "    maps={}\n",
        "    for clust in range(n_clusters):\n",
        "        maps[clust]=(fl==clust)\n",
        "    c=0\n",
        "    for clust in range(n_clusters):\n",
        "        if np.sum(maps[clust])>0:\n",
        "            c+=1\n",
        "    plt.figure(figsize=(19,5))\n",
        "    d = hierarchy.dendrogram(Z,ax=axes[1], color_threshold=t)\n",
        "    axes[1].axhline(t,linestyle='--',color='red')\n",
        "    axes[1].set_ylabel('Cluster distance')\n",
        "    axes[1].axes.get_xaxis().set_visible(False)\n",
        "    axes[1].set_xlabel('Points')\n",
        "    axes[1].set_title(f'Number clusters: {c}')\n",
        "    axes[0].scatter(points[d['leaves'],0],points[d['leaves'],1], color=d['leaves_color_list'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4lTUxeSmeff1"
      },
      "source": [
        "**Properties:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "O98E0rxHeff1"
      },
      "source": [
        "If we fix a certain cutoff length $l_c$, the clusters identified $C_1,...,C_n$ at that length are such that : \n",
        "\n",
        "1- The constintute a disjoint partition of the whole dataset, i.e. they are mutually non intersecting aand each point belongs to a cluster \n",
        "\n",
        "2- The distance between two clusters $d(C_1,C_j)$ is larger than $l_c$ for each $i,j$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd9WGjVQ951B"
      },
      "source": [
        "## 4. Gaussian mixtures (soft clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "baYX-IKOB461"
      },
      "source": [
        "### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "jkueVtkf951D"
      },
      "source": [
        "K-Means is a modelling procedure which is biased towards clusters of circular shape and therefore does not always work perfectly, as the following examples show:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "S_kqTtgi951F"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th1()\n",
        "clusterer = KMeans(n_clusters=3, random_state=10)\n",
        "cluster_labels=clusterer.fit_predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.title('K-Means')\n",
        "plt.xlim(-6,6)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "bRsc4ZOV951O"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th2()\n",
        "clusterer = KMeans(n_clusters=2, random_state=10)\n",
        "cluster_labels=clusterer.fit_predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title('K-Means')\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Gaussian mixture model is able to fit these kinds of clusters. In a Gaussian mixture model, each point of the distribution is supposed to be a random point from a distribution given by a superposition of Gaussians:\n",
        "\n",
        "$$f(\\mathbf{x})=\\sum_c \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$$\n",
        "\n",
        "Each Gaussian describes the shape of each group, that may be also non spherical or tilted.\n",
        "\n",
        "This density has the following interpretation. Suppose each data point $\\hat{X}$ results from the following 2-step procedure:\n",
        "\n",
        "1- First choose a random number between $\\{1,...,N_c\\}$ using some prior probabilities $\\{\\pi_1,...\\pi_{N_c}\\}$. This random variable says to what group a the point belong.\n",
        "\n",
        "2- Now we extract the features corresponding to that point $\\hat{X}$ using the corresponding density $N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$.\n",
        "\n",
        "The parameters $\\{\\pi_c,\\mathbf{\\mu_c},\\mathbf{\\Sigma_c}\\}$ are fitted from the data using a minimization procedure (maximum likelihood via the EM algorithm) and $N_c$ is the chosen number of clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wqOLzPso952G"
      },
      "source": [
        "**Output of a GM computation:** \n",
        "\n",
        "Given the features of a new point we can reverse engineer the probability that this point was extracted one group or the other.\n",
        "\n",
        "- *Cluster probabilities:* A gaussian mixtures model is an example of soft clustering, where each data point $p$ does not get assigned a unique cluster, but a distribution over clusters $f_p(c), c=1,...,N_c$. \n",
        "\n",
        "- *AIC/BIC:* after each clustering two numbers are returned. These can be used to select the optimal number of Gaussians to be used, similar to the Silhouette score. ( AIC and BIC consider both the likelihood of the data given the parameters and the complexity of the model related to the number of Gaussians used ). The lowest AIC or BIC value is an indication of a good fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "fdddJhgl952H"
      },
      "source": [
        "### Sklearn: implementation and usage of Gaussian mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PCyazHHp952W"
      },
      "source": [
        "First of all, we see how the Gaussian model behaves on our original example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "MTu1jVUr952Y",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "points=km_load_th1()\n",
        "\n",
        "aic=[]\n",
        "bic=[]\n",
        "sil=[]\n",
        "\n",
        "for i_comp in range(2,6):\n",
        "    plt.figure()\n",
        "    plt.title(str(i_comp))\n",
        "    clf = GaussianMixture(n_components=i_comp, covariance_type='full')\n",
        "    clf.fit(points)\n",
        "    cluster_labels=clf.predict(points)\n",
        "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "    print(i_comp,clf.aic(points),clf.bic(points))\n",
        "    score=silhouette_score(points,cluster_labels)\n",
        "    aic.append(clf.aic(points))\n",
        "    bic.append(clf.bic(points))\n",
        "    sil.append(score)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "iVTi69ZS952o"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(2,6),aic,'-o')\n",
        "plt.title('aic')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(2,6),bic,'-o')\n",
        "plt.title('bic')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(2,6),sil,'-o')\n",
        "plt.title('silhouette')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "W_RrKQ0W952z"
      },
      "source": [
        "So in this case we get a comparable result, and also the probabilistic tools agree with the Silhouette score ! Let's see how the Gaussian mixtures behave in the examples where K-means was failing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "cKmBGlDr9521"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th1()\n",
        "clf = GaussianMixture(n_components=3, covariance_type='full')\n",
        "clf.fit(points)\n",
        "cluster_labels=clf.predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.title('K-Means')\n",
        "plt.xlim(-6,6)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4ubCBvfj952-"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th2()\n",
        "clf = GaussianMixture(n_components=2, covariance_type='full')\n",
        "clf.fit(points)\n",
        "cluster_labels=clf.predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "9O3ArMG6eff4"
      },
      "source": [
        "# Final comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3EbOD7_Eeff5"
      },
      "source": [
        "We covered here the most basic clustering techniques, showcasing different behaviors. For real like projects, there are also other algorithms that could be taken into consideration, e.g. a popular algorithm is HDBSCAN:\n",
        "\n",
        "- HDBSCAN : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html\n",
        "\n",
        "whose main characteristics is that it defines automatically an optimal number of clusters. HDBSCAN can fit clusters of also weirder shapes. It is often used in combination to a non-linear dimensionality reduction methodology like UMAP.\n",
        "\n",
        "Ultimately, the best way to measure the quality of a cluster is by visual inspecting its elements and see if they make sense domain-wise."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dE4fVdZG953V",
        "uHfT21bLjv27",
        "pUus-6PW95xA",
        "T0sSzs5h95xC",
        "gOmoafxD95xT",
        "jInWo3Uk95z_",
        "v9TSuObo950Z",
        "qd9WGjVQ951B",
        "baYX-IKOB461",
        "fdddJhgl952H",
        "IIykj5dp953I",
        "oAAjJuenj1u0",
        "5AXSFimKkt91",
        "C-mH0li3kzNi",
        "8QZtmo6Vk2rp",
        "u62hCoFbklaW",
        "yrwaonuB4F8m",
        "aOk-BLOFeV61"
      ],
      "name": "Course_3",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
