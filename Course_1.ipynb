{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neworldemancer/DSF5/blob/master/Course_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_5oRe0SXilM"
      },
      "source": [
        "# Introduction to machine learning & Data Analysis\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Data Science Lab, University Of Bern, 2023\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxiIczg1s1k"
      },
      "source": [
        "# What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll5e8N9SVwVa"
      },
      "source": [
        "## Why Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6tHZQCywhGB"
      },
      "source": [
        "\n",
        "\n",
        "1.   \n",
        "2.   \n",
        "3.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PbjhPxLmsI4"
      },
      "source": [
        "## Learning from data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsd1MyT9eIdW"
      },
      "source": [
        "Unlike classical algorithms, created by a human to analyze some data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtoqE5XO3L1j"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_1.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkelBk_L6ENw"
      },
      "source": [
        "in machine learning the data itself is used for to define the algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6X7b0AG6ENw"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBrquDTR6ENw"
      },
      "source": [
        "\n",
        "The boundaries are sometimes a bit fuzzy between classical and machine learning algorithm.\n",
        "\n",
        "In fact when we create algorithms, the problem in hand (namely the data  related to the problem), drives us to choose one or another algorithm. And we then tune it, to perform well on a task in hand.\n",
        "\n",
        "There are three macro-areas of ML algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHEUHwrN6ENx"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_4.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to8vOJeC1xjE"
      },
      "source": [
        "In this course we will explore the foundations of the first two macro-areas, the ones most often used in applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recent examples. \n",
        "\n",
        "1. Hugginface https://github.com/huggingface/transformers and ChatGPT https://chat.openai.com/ from openAI unleashed the power of transformer architectures https://arxiv.org/abs/1706.03762.\n",
        "\n",
        "2. Segment anything https://github.com/facebookresearch/segment-anything\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks2.jpg\" width=\"40%\"/>\n",
        "\n",
        "3. Mediapipe https://mediapipe-studio.webapps.google.com/home\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_5.png\" width=\"40%\"/>\n",
        "\n",
        "All these applications would have been impossible without using enormous amounts of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROFPolZpm21t"
      },
      "source": [
        "## Classification vs Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70_dMCX340Rm"
      },
      "source": [
        "The two main tasks handled by (supervised) ML is regression and classification.\n",
        "In regression we aim at modeling the relationship between the system's response (dependent variable) and one or more explanatory variables (independent variables).\n",
        "\n",
        "Examples of regression would be predicting the temperature for each day of the year, or expenses of the household as a function of the number of children and adults.\n",
        "\n",
        "In classification the aim is to identify what class does a data-point belong to. For example, the species or the iris plant based on the size of its petals, or whether an email is spam or not based on its content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBXGs0xRERuv"
      },
      "source": [
        "## Performance measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx37P09Vkepw"
      },
      "source": [
        "1. Regression:\n",
        "* Mean Square Error: $\\textrm{MSE}=\\frac{1}{n}\\sum_i(y_i - \\hat y(x_i))^2$\n",
        "* Mean Absolute Error: $\\textrm{MAE}=\\frac{1}{n}\\sum_i|y_i - \\hat y(x_i)|$\n",
        "* Median Absolute Deviation: $\\textrm{MAD}=\\textrm{median}(|y_i - \\hat y(x_i)|)$\n",
        "* Fraction of the explained variance: $R^2=1-\\frac{\\sum_i(y_i - \\hat y(x_i))^2}{\\sum_i(y_i - \\bar y)^2}$,\n",
        "\n",
        "where  $\\hat y(x_i)$ is the prediction provided by the model for input $x_i$, $y_i$ is the exact value and $\\bar y=\\frac{1}{n}\\sum_i y_i$.\n",
        "\n",
        "2. Classification:\n",
        "* Confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSH3blOw36jz"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/confusion_mtr.png\" width=\"46%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2gGVJyfdUJ"
      },
      "source": [
        "* Accuracy $=\\frac{\\textrm{TP} + \\textrm{TN}}{\\textrm{TP} + \\textrm{FP} + \\textrm{FN} + \\textrm{TN}}$\n",
        "* Precision $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}}$\n",
        "* Recall $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}}$\n",
        "* F1 $=\\frac{2}{\\frac{1}{\\textrm{Precision}} + \\frac{1}{\\textrm{Recall}} }$\n",
        "* Threat score (TS), or Intersection over Union: $\\mathrm{IoU}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}+\\mathrm{FP}}$\n",
        "\n",
        "\n",
        "During model optimization the used measure in most cases must be differentiable. To this end usually some measure of similarities of distributions are employed (e.g. cross-entropy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD6zwuTHiYKA"
      },
      "source": [
        "## Actual aim: Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdBuAT-46ENx"
      },
      "source": [
        "In supervised machine learning we distinguish two faces: `training` and `testing`. To measure model performance in an unbiassed way, we need to use different data to test the model than the data that the model was trained on.\n",
        "\n",
        "When the test set contains information about the training set that can spoil the resulting metrics we talk about `data leakage` or `information leakage`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxYGpTmk6ENx"
      },
      "source": [
        "Therefore we often use the 'train-test' split: e.g. 20% of all available dataset is reserved for model performance test, and the remaining 80% is used for actual model training (these percentages may vary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsD3FQS4JP7"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_1.png\" width=\"35%\"/>\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVSRftm8X1m1"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJn0ilgOS8F",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Scikit-learn (formerly scikits.learn and also known as sklearn) is a free\n",
        "# software machine learning library for the Python programming language.\n",
        "# It features various classification, regression and clustering algorithms,\n",
        "# and is designed to interoperate with the Python numerical and scientific\n",
        "# libraries NumPy and SciPy. (from wiki)\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# common visualization module\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# numeric library\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from time import time as timer\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg0LDjc5nECH"
      },
      "outputs": [],
      "source": [
        "pip install dtreeviz==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gFMn8yunHDI"
      },
      "outputs": [],
      "source": [
        "from dtreeviz.trees import dtreeviz # remember to load the package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y7aMevU3Ug8"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pclZR6uFklf_"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wxOrdWko8W"
      },
      "source": [
        "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQgU5I-lEll"
      },
      "source": [
        "## 1. Synthetic linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGfWOWRjlWPa"
      },
      "outputs": [],
      "source": [
        "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
        "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
        "\n",
        "  w = w or np.random.uniform(0.1, 10, n_d)\n",
        "  b = b or np.random.uniform(-10, 10)\n",
        "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
        "\n",
        "  print('true slopes: w =', w, ';  b =', b)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RLYxGy_nBZG"
      },
      "outputs": [],
      "source": [
        "x, y = get_linear(n_d=1, sigma=1)\n",
        "plt.plot(x[:, 0], y, '*')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ODDOp4nX4S"
      },
      "outputs": [],
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=100)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=10)\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_zlabel('y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ5rjq7fIe8Q"
      },
      "source": [
        "## 2. House prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-45usskInlD"
      },
      "source": [
        "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVv2ID96IyN0"
      },
      "outputs": [],
      "source": [
        "def house_prices_dataset(return_df=False, return_df_xy=False, price_max=400000, area_max=40000):\n",
        "  path = 'data/AmesHousing.csv'\n",
        "\n",
        "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False,  )\n",
        "\n",
        "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "  useful_fields = ['LotArea',\n",
        "                  'Utilities', 'OverallQual', 'OverallCond',\n",
        "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
        "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
        "                  'FullBath', 'HalfBath',\n",
        "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
        "                  'Functional','PoolArea',\n",
        "                  'YrSold', 'MoSold'\n",
        "                  ]\n",
        "  target_field = 'SalePrice'\n",
        "\n",
        "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
        "\n",
        "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
        "                  'LotFrontage': {'NA':0},\n",
        "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
        "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
        "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
        "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
        "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'CentralAir':  {'N':0, 'Y': 1},\n",
        "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
        "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
        "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
        "                  }\n",
        "\n",
        "  df_X = df[useful_fields].copy()\n",
        "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
        "  df_Y = df[target_field].copy()\n",
        "\n",
        "  x = df_X.to_numpy().astype(np.float32)\n",
        "  y = df_Y.to_numpy().astype(np.float32)\n",
        "\n",
        "  if price_max>0:\n",
        "    idxs = y<price_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  if area_max>0:\n",
        "    idxs = x[:,0]<area_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  return (x, y, df) if return_df else ((x, y, (df_X, df_Y)) if return_df_xy else (x,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqWU0eHts1RM"
      },
      "outputs": [],
      "source": [
        "x, y, df = house_prices_dataset(return_df=True)\n",
        "print(x.shape, y.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDtzVS-1Mxxe"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91nj7znzMEpA"
      },
      "outputs": [],
      "source": [
        "plt.plot(x[:, 0], y, '.')\n",
        "plt.xlabel('area, sq.ft')\n",
        "plt.ylabel('price, $');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNxkPdNB4L"
      },
      "source": [
        "## 3. Blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8wXhleONKgZ"
      },
      "outputs": [],
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKcmdcZf0VO8"
      },
      "outputs": [],
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]  # affine transformation matrix\n",
        "x = np.dot(x, transformation)               # applied to point coordinated to make blobs less separable\n",
        "\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S1jwU4cXQX4"
      },
      "source": [
        "## 4. MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2u82UQ5XQX4"
      },
      "source": [
        "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaNaGGOkXQX5"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlUY5gl8XQX7"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtYtGEDdXQX8"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfbaOgfYNsq"
      },
      "source": [
        "## 5. Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzzOS7YYTru"
      },
      "source": [
        "`Fashion-MNIST` is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcV2gzmuYljJ"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPw6-GoPbT6U"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHFd0sFHY4Li"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2LkoWfZEi4g"
      },
      "outputs": [],
      "source": [
        "fmnist_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHEA0tCLagoV"
      },
      "source": [
        "Each of the training and test examples is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHRXds9U9134"
      },
      "source": [
        "# `scikit-learn` interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2toQKrAzH_U"
      },
      "source": [
        "In this course we will primarily use the `scikit-learn` module.\n",
        "You can find extensive documentation with examples in the [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
        "\n",
        "The module contains A LOT of different machine learning methods, and here we will cover only few of them. What is great about `scikit-learn` is that it has a uniform and consistent interface.\n",
        "\n",
        "All the different ML approaches are implemented as classes with a set of same main methods:\n",
        "\n",
        "1. `fitter = ...`: Create fitter object.\n",
        "2. `fitter.fit(x, y[, sample_weight])`: Fit model to predict from list of smaples `x` a list of target values `y`.\n",
        "3. `y_pred = fitter.predict(X)`: Predict using the trained model.\n",
        "4. `s = fitter.score(x, y[, sample_weight])`: Obtain a relevant performance measure of the trained model.\n",
        "\n",
        "This allows one to easily replace one approach with another and find the best one for the problem at hand, by simply using a regression/classification object of another class, while the rest of the code can remain the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLR5-eQ2vtz"
      },
      "source": [
        "It is useful to know that generally in scikit-learn the input data is represented as a matrix $X$ of dimensions `n_samples x n_features` (also called the `design matrix`), whereas the supervised labels/values are stored in a matrix $Y$ of dimensions `n_samples x n_target` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4qgOdz7Yyeb"
      },
      "source": [
        "# 1.Linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh6lII-Hz8u-"
      },
      "source": [
        "In many cases the scalar value of interest - dependent variable - is (or can be approximated as) linear combination of the independent variables.\n",
        "\n",
        "In linear regression the estimator is searched in the form: $$\\hat{y}(\\bar{x} | w_0,\\bar{w}) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
        "\n",
        "The parameters $\\bar{w} = (w_1,..., w_p)$ and $w_0$ are designated as `coef_` and `intercept_` in `sklearn`.\n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/linear_model.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlf6_berQ1vq"
      },
      "source": [
        "## 1. Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zatxRr8bOuTs"
      },
      "source": [
        "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) fits a linear model with coefficients $\\bar{w} = (w_1,..., w_p)$ and $w_0$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "Mathematically it solves a problem of the form:  $$(w^{opt}_0,\\bar{w}^{opt}) = \\arg min_{w_0,\\bar{w}} \\sum_i \\left(\\hat{y}(\\bar{x_i} | w_0,\\bar{w})-y_i\\right)^2$$\n",
        "\n",
        "The function:\n",
        "\n",
        "$$L(w,\\bar{w}) \\equiv \\sum_i \\left(\\hat{y}(\\bar{x_i} | w_0,\\bar{w})-y_i\\right)^2$$\n",
        "\n",
        "is called the `loss function` for linear regression and we say that during training/fitting the loss function is `minimized`.\n",
        "\n",
        "In terms of the design matrix $X_{i,p}$, where $i$ ranges over the number of samples and $p$ across the number of features:\n",
        "\n",
        "$$L(w,\\bar{w})=\\sum_i \\left((w_0 + \\sum_p X_{i,p}w_p)-y_i\\right)^2$$\n",
        "\n",
        "This makes it explicit that the function to minimize is quadratic in $w$ and an analytical solution is therefore available (no numerical optimization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqh7XwGkNg6r"
      },
      "outputs": [],
      "source": [
        "x, y = get_linear(n_d=1, sigma=3, n_points=30)  # p==1, 1D input\n",
        "plt.scatter(x, y);\n",
        "plt.xlabel('x')\n",
        "plt.xlabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFawJfQJOKX3"
      },
      "outputs": [],
      "source": [
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diHNLTNMOek5"
      },
      "outputs": [],
      "source": [
        "w, w0 = reg.coef_, reg.intercept_\n",
        "print(w, w0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyeHY3bxPYSF"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y, marker='*', label='data points')\n",
        "x_f = np.linspace(x.min(), x.max(), 10)\n",
        "y_f = w0 + w[0] * x_f\n",
        "plt.plot(x_f, y_f, label='fit', c='r')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNX-5gYOIi40"
      },
      "outputs": [],
      "source": [
        "# mse\n",
        "np.std(y - reg.predict(x))  # or use metrics.mean_squared_error(..., squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID0Hdzx0NvxF"
      },
      "outputs": [],
      "source": [
        "# R2\n",
        "reg.score(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rg2_DZCHgJE"
      },
      "source": [
        "Let's try 2D input.\n",
        "Additionally, here we will split the whole dataset into training and test subsets using the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK5MILosSI7d"
      },
      "outputs": [],
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=1000, sigma=5)\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x_train[:,0], x_train[:,1], y_train, marker='x', s=40)\n",
        "ax.scatter(x_test[:,0], x_test[:,1], y_test, marker='+', s=80)\n",
        "\n",
        "xx0 = np.linspace(x[:,0].min(), x[:,0].max(), 10)\n",
        "xx1 = np.linspace(x[:,1].min(), x[:,1].max(), 10)\n",
        "xx0, xx1 = [a.flatten() for a in np.meshgrid(xx0, xx1)]\n",
        "xx = np.stack((xx0, xx1), axis=-1)\n",
        "yy = reg.predict(xx)\n",
        "ax.plot_trisurf(xx0, xx1, yy, alpha=0.25, linewidth=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW5GiLlhS3Y8"
      },
      "outputs": [],
      "source": [
        "# mse\n",
        "print('train mse =', np.std(y_train - reg.predict(x_train)))\n",
        "print('test mse =', np.std(y_test - reg.predict(x_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Fb1zb5S3ZG"
      },
      "outputs": [],
      "source": [
        "# R2\n",
        "print('train R2 =', reg.score(x_train, y_train))\n",
        "print('test R2 =', reg.score(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI6s2Amob48j"
      },
      "source": [
        "## EXERCISE 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRi8SPiMb9FM"
      },
      "source": [
        "Use linear regression to fit house prices dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaQVHyvPcHW2"
      },
      "outputs": [],
      "source": [
        "# 1. make train/test split\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate MSE, MAD, and R2 on train and test datasets\n",
        "\n",
        "# 4. plot y vs predicted y for test and train parts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZX9MQlORLfY"
      },
      "source": [
        "## 2. Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRUwQD5UR0Vf"
      },
      "source": [
        "***-binary case***\n",
        "\n",
        "Logistic regression, despite its name, is a linear model for classification rather than regression. Examples of binary output can be \\{sold, not sold \\} when predicting whether a product will be sold or not, or \\{failed, passed\\} to predict the probability of a student of failing a class. It is custom to map the mutually exclusive classes to targets $y_i \\in \\{0,1\\}$, called the positive and negative class respectively.\n",
        "\n",
        "In binary logistic regression the probability $p$ of a point belonging to the positive is modelled as: $$p =\\sigma (w_0 + w_1 x_1 + ... + w_p x_p)$$, where the sigmoid function is defined as:\n",
        "\n",
        "$$\\sigma(x)=\\frac{1}{1+e^{-x}} $$\n",
        "\n",
        "The sigmoid function maps the real numbers output of a generic linear function to [0,1].\n",
        "\n",
        "The parameters $w$ are in this case trained to minimize the following loss function:\n",
        "\n",
        "$$L(w_0,\\bar{w})=-\\sum_i y_i \\log(p_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y3zzbwc6EN1"
      },
      "source": [
        "***-more classes***\n",
        "\n",
        "We have a `multiclass` model when each sample must belong to one of several mutually exclusive classes, e.g. to predict the reaction to a treatment between the classes \\{negative, not significant, positive \\}.\n",
        "\n",
        "We introduce the one-hot representation of the output $y_i^{OH}(c)=1$ if sample $i$ belongs to class $c$ and $0$ otherwise. This generates from a multiclass label like $y_i \\in \\{cat, dog, other\\}$ three binary labels: $y_i^{\\text{OH}}(\\text{dog}),y_i^{\\text{OH}}(\\text{cat}),y_i^{\\text{OH}}(\\text{other})  \\in \\{0,1\\}$.\n",
        "\n",
        "Two options are available for solving multiclass problems.\n",
        "\n",
        "* One-versus-rest (`ovr`) performs several independent fits of binary logistic regressions models, one for each class $c$:\n",
        "$$p'_c =\\sigma (w_{c,0} + w_{c,1} x_1 + ... + w_{c,p} x_{p})$$\n",
        "minimizing the binary loss function for each class separately:\n",
        "\n",
        "$$L_c(w_0,\\bar{w})=-\\sum_i y_i^{\\text{OH}}(c) \\log(p'_{i,c})$$\n",
        "\n",
        "Note that if $y_i^{\\text{OH}}(c)=0$ than sample i must belong to one of the other classes (hence the name).\n",
        "\n",
        "Since each model is trained independently the probabilitities $p'_c$ do not need to sum to one. This constrained (if needed) is often imposed a-posteriori:\n",
        "$$p_c = \\frac{p'_{c}}{\\sum_{c'}{p'_{c'}}}$$\n",
        "\n",
        "* The multinomial (`multinomial`) logistic regression evaluates the so-called logits and then aggregates them using the softmax operator:\n",
        "\n",
        "$$\\text{logits}_{c}=w_{c,0} + w_{c,1} x_1 + ... + w_{c,p} x_{p}$$\n",
        "$$p_c = \\text{softmax}(\\text{logits})_c=\\frac{e^{\\text{logits}_c}}{\\sum_{c'} e^{\\text{logits}_{c'}}}$$\n",
        "\n",
        "Note that the probabilities returned by the models are already normalized before training. All the parameters are then trained simultaneously and minimize the loss function:\n",
        "\n",
        "$$L(\\{w_{0,c}\\},\\{\\bar w_{c}\\} )=-\\sum_i \\left(\\sum_c y_i^{OH}(c)\\log p_{i,c}\\right)$$\n",
        "\n",
        "NOTE 1: the choices done here may seem ad-hoc. They are the result of several efforts in data-analysis research. E.g. the loss function has its derivation into probability theory and the maximum likelyhood framework.\n",
        "\n",
        "NOTE 2: if we have a problem with multiple classes but not exclusive, this is called a `multilabel` setting. This can be tackled by logistic regression fitting individual binary models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BnNRNDj-zbE"
      },
      "outputs": [],
      "source": [
        "# routine for coloring 2d space according to class prediction\n",
        "\n",
        "def plot_prediction_2d(x_min, x_max, y_min, y_max, classifier, ax=None, model=None):\n",
        "  \"\"\"\n",
        "  Creates 2D mesh, predicts class for each point on the mesh, and visualises it\n",
        "  \"\"\"\n",
        "\n",
        "  mesh_step = .02  # step size in the mesh\n",
        "  x_coords = np.arange(x_min, x_max, mesh_step) # coordinates of mesh colums\n",
        "  y_coords = np.arange(y_min, y_max, mesh_step) # coordinates of mesh rows\n",
        "\n",
        "  # create mesh, and get x and y coordinates of each point point\n",
        "  # arrenged as array of shape (n_mesh_rows, n_mesh_cols)\n",
        "  mesh_nodes_x, mesh_nodes_y = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # prepare xy pairs for prediction: matrix of size (n_mesh_rows*n_mesh_cols, 2)\n",
        "  mesh_xy_coords = np.stack([mesh_nodes_x.flatten(),\n",
        "                             mesh_nodes_y.flatten()], axis=-1)\n",
        "\n",
        "  # obtain class for each node\n",
        "  if model=='xgboost':\n",
        "    mesh_xy_coords = xgb.DMatrix(mesh_xy_coords)\n",
        "    mesh_nodes_class = classifier.predict(mesh_xy_coords)\n",
        "  else:\n",
        "    mesh_nodes_class = classifier.predict(mesh_xy_coords)\n",
        "  \n",
        "\n",
        "  # reshape to the shape (n_mesh_rows, n_mesh_cols)==mesh_nodes_x.shape for visualization\n",
        "  mesh_nodes_class = mesh_nodes_class.reshape(mesh_nodes_x.shape)\n",
        "\n",
        "  # Put the result into a color countour plot\n",
        "  ax = ax or plt.gca()\n",
        "  ax.contourf(mesh_nodes_x,\n",
        "              mesh_nodes_y,\n",
        "              mesh_nodes_class,\n",
        "              cmap='Pastel1', alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-GFZDsykukn"
      },
      "outputs": [],
      "source": [
        "# routine for plotting 3d probability hyperplanes\n",
        "\n",
        "def plot_hyperplanes_3d(x_min, x_max, y_min, y_max, classifier):\n",
        "  \"\"\"\n",
        "  Creates 2D mesh, predicts class for each point on the mesh, and visualises it\n",
        "  \"\"\"\n",
        "\n",
        "  mesh_step = .5  # step size in the mesh\n",
        "  x_coords = np.arange(x_min, x_max, mesh_step) # coordinates of mesh colums\n",
        "  y_coords = np.arange(y_min, y_max, mesh_step) # coordinates of mesh rows\n",
        "\n",
        "  # create mesh, and get x and y coordinates of each point point\n",
        "  # arrenged as array of shape (n_mesh_rows, n_mesh_cols)\n",
        "  mesh_nodes_x, mesh_nodes_y = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # prepare xy pairs for prediction: matrix of size (n_mesh_rows*n_mesh_cols, 2)\n",
        "  mesh_xy_coords = np.stack([mesh_nodes_x.flatten(),\n",
        "                             mesh_nodes_y.flatten()], axis=-1)\n",
        "\n",
        "  # obtain class for each node\n",
        "  coef = classifier.coef_\n",
        "  intercept = classifier.intercept_\n",
        "\n",
        "  df_dict = {\n",
        "      'x':[],\n",
        "      'y':[],\n",
        "      'p':[],\n",
        "      'c':[]\n",
        "  }\n",
        "  def fill_dict(x, y, p, c):\n",
        "    df_dict['x'].append(x)\n",
        "    df_dict['y'].append(y)\n",
        "    df_dict['p'].append(p)\n",
        "    df_dict['c'].append(c)\n",
        "\n",
        "  for c in classifier.classes_:\n",
        "    coef_c = coef[c]\n",
        "    intercept_c = intercept[c]\n",
        "    for x, y in mesh_xy_coords:\n",
        "      p = x*coef_c[0] + y*coef_c[1] + intercept_c\n",
        "      fill_dict(x, y, p, c)\n",
        "  df = pd.DataFrame(df_dict)\n",
        "  fig = px.scatter_3d(df, x='x', y='y', z='p', color='c', size_max=1)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bJHWawkq0ev"
      },
      "outputs": [],
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "x, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "x = np.dot(x, transformation)\n",
        "\n",
        "for multi_class in ('multinomial', 'ovr'):\n",
        "    # do fit\n",
        "    clf = linear_model.LogisticRegression(solver='sag', max_iter=100,\n",
        "                             multi_class=multi_class, )\n",
        "    clf.fit(x, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training accuracy : %.3f (%s)\" % (clf.score(x, y), multi_class))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = x[:, 0]\n",
        "    x_1 = x[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=clf)\n",
        "\n",
        "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color, cmap=plt.cm.Paired,\n",
        "                    edgecolor='gray', s=30, linewidth=0.2)\n",
        "\n",
        "    # Plot the three one-against-all classifiers\n",
        "    coef = clf.coef_\n",
        "    intercept = clf.intercept_\n",
        "\n",
        "    def plot_hyperplane(c, color):\n",
        "        def line(x0):\n",
        "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
        "        plt.plot([x_min, x_max], [line(x_min), line(x_max)],\n",
        "                 ls=\"--\", color=color)\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        plot_hyperplane(i, color)\n",
        "\n",
        "    plt.show()\n",
        "    #plot_hyperplanes_3d(x_min, x_max, y_min, y_max, classifier=clf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ69XKdbZcA3"
      },
      "source": [
        "## EXERCISE 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__9jcqXzZaQp"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304Ul40adUT2"
      },
      "source": [
        "We will reshape 2-d images to 1-d arrays for use in scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtD8C8_4a7dP"
      },
      "outputs": [],
      "source": [
        "n_train = len(train_labels)\n",
        "x_train = train_images.reshape((n_train, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJj7ofWD_Wp2"
      },
      "source": [
        "Now use a multinomial logistic regression classifier, and measure the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeIKcMeV_rmk"
      },
      "outputs": [],
      "source": [
        "# 1. Create classifier\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate accuracy on train and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-CGSS2OZKHD"
      },
      "source": [
        "\n",
        "# 2. Trees & Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxtv48o-F1Ku"
      },
      "source": [
        "## 1. Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l582Sr0_WGXj"
      },
      "source": [
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning **simple** decision rules inferred from the data features.\n",
        "\n",
        "They are fast to train, easily interpretable, capture non-linear dependencies, and require small amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF3CCNq26EN2"
      },
      "source": [
        "We will see that creation trees create a partition of the feature space $X$ into subregions $R_i,i=1..N_r$, this partition being described by a tree structure. Predictions will be made with the following procedure. Given a new test point $x$:\n",
        "\n",
        "1. Assign $x$ to the region it belongs, e.g. $R_k$\n",
        "2. For classification, make a majiority vote using the training points belonging to $R_k$. For regression, evaluate the mean of the target over all training points belonging to $R_k$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz5raIG_WQfg"
      },
      "outputs": [],
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "dtcs = []\n",
        "for depth in (1, 2, 3, 4):\n",
        "    # do fit\n",
        "    dtc = tree.DecisionTreeClassifier(max_depth=depth, criterion='gini')  # 'entropy'\n",
        "    dtcs.append(dtc)\n",
        "    dtc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (depth=%d)\" % (dtc.score(X, y), depth))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2,  figsize=(14,7), dpi=300)\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=dtc, ax=ax[0])\n",
        "\n",
        "    ax[0].set_title(\"Decision surface of DTC (%d)\" % depth)\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = \"rbg\"\n",
        "    for i, color in zip(dtc.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        ax[0].scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "    with plt.style.context('classic'):\n",
        "      tree.plot_tree(dtc, ax=ax[1]);\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IgYhWghyUhL"
      },
      "source": [
        "But how is the tree constructed? We distinguish here classification and regression:\n",
        "\n",
        "***-classification***\n",
        "\n",
        "Given any sample $S$ of points in feature space $x_i,i=1...N$, assigned to classes $c_i,i=1..N, c_i \\in {1,..,N_c}$, we can evaluate the proportions $p_c=\\frac{N_c}{N}$ where $N_c$ is the number of points belonging to class $c$.\n",
        "\n",
        "* the Gini index is defined as:\n",
        "$$G(S) = 1 - \\sum_c {p_c^2}, $$\n",
        "\n",
        "* the Entropy is:\n",
        "$$E(S) =  - \\sum_c {p_c \\times log(p_c)}, $$\n",
        "\n",
        "These are defined `impurity measures`.\n",
        "\n",
        "The optimization is performed in a greedy manner, one split at a time. First a coordinate is chosen and a cutoff $a$. Points $i$ such that $x_{i,k}<a$ go to the left child node, points If $x_{i,k} \\ge a$ go to the right child node. We define $S_{<}$, $S_{>}$ as the set of points belonging to the left/right child and $N_{<}$ and $N_{>}$ the corresponding number of points. The information gain of the split is defined as:\n",
        "\n",
        "$$ IG = E(S) - \\frac{N_{<}}{N} \\times E(S_{<}) - \\frac{N_{>}}{N} \\times E(S_{>})$$\n",
        "\n",
        "Same exepression holds for the Gini index. At each stage the split with the higher information gain is preferred. This favors that the most relevant features are used on the top of the tree and truncating/pruning a tree reduces overfitting.\n",
        "\n",
        "***-regression***\n",
        "\n",
        "The regression framework has a lot in common with classification but we start with sample $S$ of points in feature space $x_i,i=1...N$ and their targets $y_i,y_i \\in \\mathbf{R}$. Instead of the impurity measures we use the sample variance as a criterion:\n",
        "\n",
        "$$V(S)=\\frac{1}{N} \\sum_i (y_i - \\bar y)^2$$\n",
        "\n",
        "Note that in the regression case we have a simplication of prefactors. If we call $y_<$ the mean of the left points and $y_>$ the mean of the right points the role of the information gain here is taken by:\n",
        "\n",
        "$$\\frac{N_{<}}{N} \\times V(S_{<}) + \\frac{N_{>}}{N} \\times V(S_{>})=\\frac{1}{N}\\left(\\sum_{i \\in S_<} (y_i-y_<)^2+\\sum_{i \\in S_>} (y_i-y_>)^2\\right)$$\n",
        "\n",
        "So the information gain is replaced by the `reduction in variance` after predicting a different mean for the points belonging to the left and right children.\n",
        "\n",
        "These are also known as the CART (classification and regression trees) splitting criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRsuFUM2nlP5"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
        "\n",
        "for i, dtc in enumerate(dtcs):\n",
        "  viz = dtreeviz(dtc, X, y, feature_names=['x[0]', 'x[1]'])\n",
        "  viz.scale=1.2\n",
        "  display(viz)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJK1XADt94uK"
      },
      "source": [
        "Additionally we can directly inspect relevance of the input features for the classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxEFpn2P9Uek"
      },
      "outputs": [],
      "source": [
        "plt.bar(['x_0','x_1'], dtcs[2].feature_importances_)\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('feature importance')\n",
        "plt.ylim(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHZ-hHGuY5aG"
      },
      "source": [
        "## 2. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETTKyFmwTae"
      },
      "source": [
        "The `sklearn.ensemble` provides several ensemble algorithms. RandomForest is an averaging algorithm based on randomized decision trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction.\n",
        "\n",
        "The prediction of the ensemble is given as the averaged prediction of the individual classifiers (regression) or by majority voting (classification). E.g. for regression:\n",
        "\n",
        "$$ RF(x) = \\frac{1}{N_\\text{trees}}\\sum_{i=1}^{N_\\text{trees}} Tree_i(x)$$\n",
        "\n",
        "Individual decision trees typically exhibit high variance and tend to overfit.\n",
        "In random forests:\n",
        "* each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
        "* when splitting each node during the construction of a tree, the best split is found from a random subset of features, according to `max_features` parameter.\n",
        "\n",
        "The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant, hence yielding an overall better model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4hfPUqSZCbH"
      },
      "outputs": [],
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for n_est in (1, 4, 50):\n",
        "    # do fit\n",
        "    rfc = ensemble.RandomForestClassifier(max_depth=4, n_estimators=n_est,)\n",
        "    rfc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (n_est=%d)\" % (rfc.score(X, y), n_est))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=rfc)\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY6EVEestysr"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=300)\n",
        "with plt.style.context('classic'):\n",
        "  tree.plot_tree(rfc.estimators_[20]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-YP_GTWsLZI"
      },
      "outputs": [],
      "source": [
        "viz = dtreeviz(rfc.estimators_[20], X, y, feature_names=[f'{i}' for i in np.arange(len(X[0]))])\n",
        "viz.scale=1.2\n",
        "display(viz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLruRG-g_pyD"
      },
      "source": [
        "For a forest we can also evaluate the feature importance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks42i4mq-RM-"
      },
      "outputs": [],
      "source": [
        "importances = np.array([e.feature_importances_ for e in rfc.estimators_])\n",
        "\n",
        "#plt.plot(importances.T, '.')\n",
        "plt.bar(['0', '1'], importances.mean(axis=0), yerr=importances.std(axis=0))\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "print('Explicetly evaluated feature importance:', importances.mean(axis=0))\n",
        "print('Importance saved in the model object:', rfc.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTNL6_57A0rq"
      },
      "source": [
        "Alternatively we can use permutation to study feature importance.\n",
        "It evaluates decrease in performance (model's `.score` or specified in `scoring` parameter) for each variable when its values is shuffled between samples.\n",
        "It can be time-consuming as requires re-evaluation for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrD0XYtI_8mQ"
      },
      "outputs": [],
      "source": [
        "p_importances = permutation_importance(rfc, X, y, n_repeats=10, n_jobs=-1)\n",
        "\n",
        "plt.bar(['0', '1'],\n",
        "        p_importances.importances_mean,\n",
        "        yerr=p_importances.importances_std)\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puQNgKN0wS7H"
      },
      "source": [
        "## 3. Boosted Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkL6_R05wez3"
      },
      "source": [
        "Another approach to the ensemble tree modeling is Boosted Decision Trees. In a boosting framework, the trees are created sequentially. This way each next tree reduces error of the ensemble, by adding corrections to previous predictions.\n",
        "\n",
        "Predictions of boosted decision trees are sequentially added, in contrast with random forest predictions:\n",
        "\n",
        "$$ Boost(x) = \\sum_{i=1}^{N_\\text{trees}} Tree_i(x)$$\n",
        "\n",
        "Predictions and classifications are treated on the same footing, by making the ensemble output logits.\n",
        "\n",
        "Usually shallow trees (with high biased) are used in boosting framework. In boosting primarily the bias is reduced, thus increasing variance.\n",
        "\n",
        "To avoid overfitting the learning rate (also called in this setting 'shrinkage') and subsampling parameter can be tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One of the most popular implementations of boosting is XGBoost:\n",
        "\n",
        "https://arxiv.org/abs/1603.02754\n",
        "\n",
        "https://xgboost.readthedocs.io/en/stable/python/python_api.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "dtrain = xgb.DMatrix(X, label=y)\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # Multiclass classification with softmax\n",
        "    'eval_metric': 'mlogloss',    # Evaluation metric (Multiclass Log Loss)\n",
        "    'num_class': 3,               # Number of classes in the target variable\n",
        "    'max_depth': 3,               # Maximum depth of trees\n",
        "    'learning_rate': 0.1,         # Learning rate\n",
        "}\n",
        "\n",
        "for n_est in [20]:\n",
        "    # do fit\n",
        "\n",
        "    model = xgb.train(params, dtrain, num_boost_round = n_est)\n",
        "    # NB: If we had done a train-test split we could print the metrics during training like this (check the API reference):\n",
        "    #model = xgb.train(params, dtrain, num_boost_round=100, \n",
        "    #              early_stopping_rounds=5, evals=[(dtest, 'eval')])\n",
        "\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier = model, model='xgboost')\n",
        "\n",
        "    plt.title(f'Decision surface of XGBoost ({n_est})')\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhycm2S6wbz"
      },
      "source": [
        "## EXERCISE 3 : Random forest classifier for FMNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I20uBLiMoURH"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "n = len(train_labels)\n",
        "x_train = train_images.reshape((n, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd59TNBWfHgX"
      },
      "source": [
        "Classify fashion MNIST images with Random Forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8waJCx33peIG"
      },
      "outputs": [],
      "source": [
        "# 1. Create classifier. As the number of features is big (784), use bigger tree\n",
        "# depth (max_depth parameter), try in range 10-500.\n",
        "\n",
        "# 2. What is the maximum number of leaves in tree of depth n?\n",
        "# To reduce variance we should avoid leaves with too litle samples. You could\n",
        "# limit the total number of tree leaves (max_leaf_nodes parameter) to 10-1000.\n",
        "# Alternatively you can use min_samples_split & min_samples_leaf\n",
        "\n",
        "# 3. Try different number of estimators (n_estimators)\n",
        "\n",
        "# 4. Fit the model\n",
        "\n",
        "# 5. Inspect training and test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dG6U6s3T95t"
      },
      "source": [
        "## EXERCISE 4: Random forest regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlypA1MaT95u"
      },
      "outputs": [],
      "source": [
        "X, y, (df_x, df_y) = house_prices_dataset(return_df_xy=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYTwkoSpT95u"
      },
      "source": [
        "Predict the house prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4wobzB_T95u"
      },
      "outputs": [],
      "source": [
        "# 1. Create regressor. (ensemble.RandomForestRegressor)\n",
        "# Try different number of estimators (n_estimators)\n",
        "\n",
        "# 2. Fit the model\n",
        "\n",
        "# 3. Inspect training and test accuracy\n",
        "\n",
        "# 4. Try to improve performance by adjusting hyperparameters.\n",
        "# How does it compare to linear model?\n",
        "\n",
        "# 5. Use dtreeviz to visualize a tree from the ensamble\n",
        "\n",
        "# 6. Study the feature importance\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "n_5oRe0SXilM",
        "9SxiIczg1s1k",
        "ll5e8N9SVwVa",
        "5PbjhPxLmsI4",
        "ROFPolZpm21t",
        "qBXGs0xRERuv",
        "AD6zwuTHiYKA",
        "NVSRftm8X1m1",
        "pclZR6uFklf_",
        "8UQgU5I-lEll",
        "FJ5rjq7fIe8Q",
        "q7CNxkPdNB4L",
        "8S1jwU4cXQX4",
        "ITfbaOgfYNsq",
        "x2NWxK0BFwyw",
        "RHRXds9U9134",
        "K4qgOdz7Yyeb",
        "Vlf6_berQ1vq",
        "zI6s2Amob48j",
        "zZX9MQlORLfY",
        "AQ69XKdbZcA3",
        "7-CGSS2OZKHD",
        "Bxtv48o-F1Ku",
        "EHZ-hHGuY5aG",
        "puQNgKN0wS7H",
        "vhhycm2S6wbz",
        "0dG6U6s3T95t"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
