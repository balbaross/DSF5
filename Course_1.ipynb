{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n_5oRe0SXilM",
        "ll5e8N9SVwVa",
        "9SxiIczg1s1k",
        "70_dMCX340Rm",
        "qBXGs0xRERuv",
        "AD6zwuTHiYKA",
        "NVSRftm8X1m1",
        "pclZR6uFklf_",
        "8UQgU5I-lEll",
        "FJ5rjq7fIe8Q",
        "q7CNxkPdNB4L",
        "8S1jwU4cXQX4",
        "ITfbaOgfYNsq",
        "RHRXds9U9134",
        "K4qgOdz7Yyeb",
        "Vlf6_berQ1vq",
        "zI6s2Amob48j",
        "zZX9MQlORLfY",
        "AQ69XKdbZcA3",
        "Bxtv48o-F1Ku",
        "EHZ-hHGuY5aG",
        "puQNgKN0wS7H",
        "vhhycm2S6wbz",
        "0dG6U6s3T95t"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_5oRe0SXilM"
      },
      "source": [
        "# Data Science Fundamentals 5\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Science IT Support, University Of Bern, 2021\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll5e8N9SVwVa"
      },
      "source": [
        "# Why Machine learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6tHZQCywhGB"
      },
      "source": [
        "\n",
        "\n",
        "1.   \n",
        "2.   \n",
        "3.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxiIczg1s1k"
      },
      "source": [
        "# What is Machine learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsd1MyT9eIdW"
      },
      "source": [
        "Unlike classical algorithms, created by human to analyze some data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtoqE5XO3L1j"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_1.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4Uq4k_ePoo"
      },
      "source": [
        "in machine learning the data itself is used for to define the algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2xIgm223vfa"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvAyI1uzfBUT"
      },
      "source": [
        "Machine learning - learnign from data:\n",
        "* performance on a task **T**\n",
        "* improves according to measure **P**\n",
        "* with experience **E**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YsgnbD32dk"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_3.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to8vOJeC1xjE"
      },
      "source": [
        "\n",
        "The boundary is a bit fuzzy.\n",
        "In fact when we create algorithms, the problem in hand, namely the data  related to the problem, drives us to choose one or another algorithm. And we then tune it, to perform well on a task in hand. ML formalized this procedure, allowing us to automate (part) of this process.\n",
        "\n",
        "In this 2-day course you will get acquainted with the basics of ML, where the approach to handling the data (the algorithm) is defined, or as we say \"learned\" from data in hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70_dMCX340Rm"
      },
      "source": [
        "## Classification vs regression.\n",
        "\n",
        "The two main tasks handled by (supervised) ML is regression and classification.\n",
        "In regression we aim at modeling the relationship between the system's response (dependent variable) and one or more explanatory variables (independent variables).\n",
        "\n",
        "Examples of regression would be predicting the temperature for each day of the year, or expenses of the household as a function of the number of children and adults.\n",
        "\n",
        "In classification the aim is to identify what class does a data-point belong to. For example, the species or the iris plant based on the size of its petals, or whether an email is spam or not based on its content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBXGs0xRERuv"
      },
      "source": [
        "## Performance measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx37P09Vkepw"
      },
      "source": [
        "1. Regression:\n",
        "* Mean Square Error: $\\textrm{MSE}=\\frac{1}{n}\\sum_i(y_i - \\hat y(\\bar x_i))^2$\n",
        "* Mean Absolute Error: $\\textrm{MAE}=\\frac{1}{n}\\sum_i|y_i - \\hat y(\\bar x_i)|$\n",
        "* Median Absolute Deviation: $\\textrm{MAD}=\\textrm{median}(|y_i - \\hat y(\\bar x_i)|)$\n",
        "* Fraction of the explained variance: $R^2=1-\\frac{\\sum_i(y_i - \\hat y(\\bar x_i))^2}{\\sum_i(y_i - \\bar y_i)^2}$, where $\\bar y=\\frac{1}{n}\\sum_i y_i$\n",
        "\n",
        "2. Classification:\n",
        "* Confusion matrix \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSH3blOw36jz"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/confusion_mtr.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2gGVJyfdUJ"
      },
      "source": [
        "* Accuracy $=\\frac{\\textrm{TP} + \\textrm{TN}}{\\textrm{TP} + \\textrm{FP} + \\textrm{FN} + \\textrm{TN}}$\n",
        "* Precision $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}}$ \n",
        "* Recall $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}}$\n",
        "* F1 $=2\\frac{\\textrm{Precision} \\cdot \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}} = \\frac{2 \\textrm{TP}}{2 \\textrm{TP} + \\textrm{FP} + \\textrm{FN}}$\n",
        "* Threat score (TS), or Intersection over Union: $\\mathrm{IoU}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}+\\mathrm{FP}}$\n",
        "\n",
        "\n",
        "During model optimization the used measure in most cases must be differentiable. To this end usually some measure of similarities of distributions are employed (e.g. cross-entropy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD6zwuTHiYKA"
      },
      "source": [
        "## Actual aim: Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsD3FQS4JP7"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_1.png\" width=\"35%\"/>\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoONru7ji3QD"
      },
      "source": [
        "To measure model performance in an unbiassed way, we need to use different data than the data that the model was trained on. For this we use the 'train-test' split: e.g. 20% of all available dataset is reserved for model performance test, and the remaining 80% is used for actual model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVSRftm8X1m1"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVJn0ilgOS8F",
        "scrolled": true
      },
      "source": [
        "# Scikit-learn (formerly scikits.learn and also known as sklearn) is a free \n",
        "# software machine learning library for the Python programming language. \n",
        "# It features various classification, regression and clustering algorithms, \n",
        "# and is designed to interoperate with the Python numerical and scientific \n",
        "# libraries NumPy and SciPy. (from wiki)\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "# common visualization module\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# numeric library\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from imageio import imread\n",
        "import pandas as pd\n",
        "from time import time as timer\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0LDjc5nECH"
      },
      "source": [
        "pip install dtreeviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gFMn8yunHDI"
      },
      "source": [
        "from dtreeviz.trees import dtreeviz # remember to load the package"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y7aMevU3Ug8"
      },
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pclZR6uFklf_"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wxOrdWko8W"
      },
      "source": [
        "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQgU5I-lEll"
      },
      "source": [
        "## 1. Synthetic linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGfWOWRjlWPa"
      },
      "source": [
        "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
        "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
        "  \n",
        "  w = w or np.random.uniform(0.1, 10, n_d)\n",
        "  b = b or np.random.uniform(-10, 10)\n",
        "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
        "\n",
        "  print('true slopes: w =', w, ';  b =', b)\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RLYxGy_nBZG"
      },
      "source": [
        "x, y = get_linear(n_d=1, sigma=0)\n",
        "plt.plot(x[:, 0], y, '*')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ODDOp4nX4S"
      },
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=100)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=40)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ5rjq7fIe8Q"
      },
      "source": [
        "## 2. House prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-45usskInlD"
      },
      "source": [
        "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVv2ID96IyN0"
      },
      "source": [
        "def house_prices_dataset(return_df=False, price_max=400000, area_max=40000):\n",
        "  path = 'data/AmesHousing.csv'\n",
        "\n",
        "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False)\n",
        "  \n",
        "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "  \n",
        "  useful_fields = ['LotArea',\n",
        "                  'Utilities', 'OverallQual', 'OverallCond',\n",
        "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
        "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
        "                  'FullBath', 'HalfBath',\n",
        "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
        "                  'Functional','PoolArea',\n",
        "                  'YrSold', 'MoSold'\n",
        "                  ]\n",
        "  target_field = 'SalePrice'\n",
        "\n",
        "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
        "\n",
        "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
        "                  'LotFrontage': {'NA':0},\n",
        "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
        "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
        "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
        "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
        "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'CentralAir':  {'N':0, 'Y': 1},\n",
        "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
        "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
        "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
        "                  }\n",
        "\n",
        "  df_X = df[useful_fields].copy()                              \n",
        "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
        "  df_Y = df[target_field].copy()\n",
        "\n",
        "  x = df_X.to_numpy().astype(np.float32)\n",
        "  y = df_Y.to_numpy().astype(np.float32)\n",
        "\n",
        "  if price_max>0:\n",
        "    idxs = y<price_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  if area_max>0:\n",
        "    idxs = x[:,0]<area_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  return (x, y, df) if return_df else (x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqWU0eHts1RM"
      },
      "source": [
        "x, y, df = house_prices_dataset(return_df=True)\n",
        "print(x.shape, y.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nj7znzMEpA"
      },
      "source": [
        "plt.plot(x[:, 0], y, '.')\n",
        "plt.xlabel('area, sq.ft')\n",
        "plt.ylabel('price, $');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNxkPdNB4L"
      },
      "source": [
        "## 3. Blobs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8wXhleONKgZ"
      },
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S1jwU4cXQX4"
      },
      "source": [
        "## 4. MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2u82UQ5XQX4"
      },
      "source": [
        "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNaGGOkXQX5"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlUY5gl8XQX7"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtYtGEDdXQX8"
      },
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout(0,0,0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfbaOgfYNsq"
      },
      "source": [
        "## 5. Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzzOS7YYTru"
      },
      "source": [
        "`Fashion-MNIST` is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcV2gzmuYljJ"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPw6-GoPbT6U"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHFd0sFHY4Li"
      },
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout(0,0,0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHEA0tCLagoV"
      },
      "source": [
        "Each of the training and test examples is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHRXds9U9134"
      },
      "source": [
        "# `scikit-learn` interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2toQKrAzH_U"
      },
      "source": [
        "In this course we will primarily use the `scikit-learn` module.\n",
        "You can find extensive documentation with examples in the [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
        "\n",
        "The module contains A LOT of different machine learning methods, and here we will cover only few of them. What is great about `scikit-learn` is that it has a uniform and consistent interface. \n",
        "\n",
        "All the different ML approaches are implemented as classes with a set of same main methods:\n",
        "\n",
        "1. `fitter = ...`: Create object.\n",
        "2. `fitter.fit(x, y[, sample_weight])`: Fit model.\n",
        "3. `y_pred = fitter.predict(X)`: Predict using the linear model.\n",
        "4. `s = score(x, y[, sample_weight])`: Return an appropriate measure of model performance.\n",
        "\n",
        "This allows one to easily replace one approach with another and find the best one for the problem at hand, by simply using another regression/classification object, while the rest of the code can remain the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLR5-eQ2vtz"
      },
      "source": [
        "It is useful to know that generally in scikit-learn the input data is represented as a design matrix $X$ of dimensions `n_samples x n_features` , whereas the supervised labels/values are stored in a matrix $Y$ of dimensions `n_samples x n_target` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4qgOdz7Yyeb"
      },
      "source": [
        "# 1.Linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh6lII-Hz8u-"
      },
      "source": [
        "In many cases the scalar value of interest - dependent variable - is (or can be approximated as) linear combination of the independent variables. \n",
        "\n",
        "In linear regression the estimator is searched in the form: $$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
        "\n",
        "The parameters $w = (w_1,..., w_p)$ and $w_0$ are designated as `coef_` and `intercept_` in `sklearn`.\n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/linear_model.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlf6_berQ1vq"
      },
      "source": [
        "## 1. Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zatxRr8bOuTs"
      },
      "source": [
        "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) fits a linear model with coefficients $w = (w_1,..., w_p)$ and $w_0$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "Mathematically it solves a problem of the form: $$\\min_{w} || X w - y||_2^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqh7XwGkNg6r"
      },
      "source": [
        "x, y = get_linear(n_d=1, sigma=3, n_points=30)  # p==1, 1D input\n",
        "plt.scatter(x, y);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFawJfQJOKX3"
      },
      "source": [
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diHNLTNMOek5"
      },
      "source": [
        "w, w0 = reg.coef_, reg.intercept_\n",
        "print(w, w0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyeHY3bxPYSF"
      },
      "source": [
        "plt.scatter(x, y, marker='*', label='data points')\n",
        "x_f = np.linspace(x.min(), x.max(), 10)\n",
        "y_f = w0 + w[0] * x_f\n",
        "plt.plot(x_f, y_f, label='fit', c='r')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNX-5gYOIi40"
      },
      "source": [
        "# mse\n",
        "np.std(y - reg.predict(x))  # or use metrics.mean_squared_error(..., squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID0Hdzx0NvxF"
      },
      "source": [
        "# R2\n",
        "reg.score(x, y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rg2_DZCHgJE"
      },
      "source": [
        "Let's try 2D input. \n",
        "Additionally, here we will split the whole dataset into training and test subsets using the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK5MILosSI7d"
      },
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=100, sigma=5)\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x_train[:,0], x_train[:,1], y_train, marker='x', s=40)\n",
        "ax.scatter(x_test[:,0], x_test[:,1], y_test, marker='+', s=80)\n",
        "\n",
        "xx0 = np.linspace(x[:,0].min(), x[:,0].max(), 10)\n",
        "xx1 = np.linspace(x[:,1].min(), x[:,1].max(), 10)\n",
        "xx0, xx1 = [a.flatten() for a in np.meshgrid(xx0, xx1)]\n",
        "xx = np.stack((xx0, xx1), axis=-1)\n",
        "yy = reg.predict(xx)\n",
        "ax.plot_trisurf(xx0, xx1, yy, alpha=0.25, linewidth=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5GiLlhS3Y8"
      },
      "source": [
        "# mse\n",
        "print('train mse =', np.std(y_train - reg.predict(x_train)))\n",
        "print('test mse =', np.std(y_test - reg.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Fb1zb5S3ZG"
      },
      "source": [
        "# R2\n",
        "print('train R2 =', reg.score(x_train, y_train))\n",
        "print('test R2 =', reg.score(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI6s2Amob48j"
      },
      "source": [
        "## EXERCISE 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRi8SPiMb9FM"
      },
      "source": [
        "Use linear regression to fit house prices dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaQVHyvPcHW2"
      },
      "source": [
        "x, y = house_prices_dataset()\n",
        "\n",
        "# 1. make train/test split\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate MSE, MAE, and R2 on train and test datasets\n",
        "\n",
        "# 4. plot y vs predicted y for test and train parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZX9MQlORLfY"
      },
      "source": [
        "## 2. Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRUwQD5UR0Vf"
      },
      "source": [
        "Logistic regression, despite its name, is a linear model for classification rather than regression. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
        "\n",
        "In logistic regression the probability $p$ of a point belonging to a class is modeled as: $$\\frac{p}{1-p} = e^{w_0 + w_1 x_1 + ... + w_p x_p}$$\n",
        "\n",
        "The binary class $\\ell_2$ penalized logistic regression minimizes the following cost function:\n",
        "$$\\min_{w, c}  \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) + \\lambda \\frac{1}{2}w^T w$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJHWawkq0ev"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "x, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "x = np.dot(x, transformation)\n",
        "\n",
        "for multi_class in ('multinomial', 'ovr'):\n",
        "    clf = linear_model.LogisticRegression(solver='sag', max_iter=100,\n",
        "                             multi_class=multi_class)\n",
        "    clf.fit(x, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training accuracy : %.3f (%s)\" % (clf.score(x, y), multi_class))\n",
        "\n",
        "    # create a mesh to plot in\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    z = z.reshape(xx.shape)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.contourf(xx, yy, z, cmap='hsv', alpha=0.05)\n",
        "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'ygr'\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x[idx, 0], x[idx, 1], c=color, cmap=plt.cm.Paired, \n",
        "                    edgecolor='gray', s=30, linewidth=0.2)\n",
        "\n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "    coef = clf.coef_\n",
        "    intercept = clf.intercept_\n",
        "\n",
        "    def plot_hyperplane(c, color):\n",
        "        def line(x0):\n",
        "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
        "        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
        "                 ls=\"--\", color=color)\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        plot_hyperplane(i, color)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ69XKdbZcA3"
      },
      "source": [
        "## EXERCISE 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__9jcqXzZaQp"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304Ul40adUT2"
      },
      "source": [
        "We will reshape 2-d images to 1-d arrays for use in scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtD8C8_4a7dP"
      },
      "source": [
        "n_train = len(train_labels)\n",
        "x_train = train_images.reshape((n_train, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJj7ofWD_Wp2"
      },
      "source": [
        "Now use a multinomial logistic regression classifier, and measure the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeIKcMeV_rmk"
      },
      "source": [
        "# 1. Create classifier\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate accuracy on train and test datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-CGSS2OZKHD"
      },
      "source": [
        "# 2. Trees & Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxtv48o-F1Ku"
      },
      "source": [
        "## 1. Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l582Sr0_WGXj"
      },
      "source": [
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning **simple** decision rules inferred from the data features.\n",
        "\n",
        "They are fast to train, easily interpretable, capture non-linear dependencies, and require small amount of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5raIG_WQfg"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "dtcs = []\n",
        "for depth in (1, 2, 3, 4):\n",
        "    dtc = tree.DecisionTreeClassifier(max_depth=depth)\n",
        "    dtcs.append(dtc)\n",
        "    dtc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (depth=%d)\" % (dtc.score(X, y), depth))\n",
        "\n",
        "    # create a mesh to plot in\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    Z = dtc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    fig, ax = plt.subplots(1, 2,  figsize=(14,7), dpi=300)\n",
        "    ax[0].contourf(xx, yy, Z, cmap='Pastel1', alpha=0.9)\n",
        "    ax[0].set_title(\"Decision surface of DTC (%d)\" % depth)\n",
        "    \n",
        "    # Plot also the training points\n",
        "    colors = \"rbg\"\n",
        "    for i, color in zip(dtc.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        ax[0].scatter(X[idx, 0], X[idx, 1], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "    tree.plot_tree(dtc, ax=ax[1]);\n",
        "\n",
        "    plt.tight_layout(0.5,0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IgYhWghyUhL"
      },
      "source": [
        "Given fraction of samples of class $i$ as $p_i$ the Gini index is:\n",
        "$$G = 1 - \\sum_i {p_i^2}, $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYpitlKczjki"
      },
      "source": [
        "text_representation = tree.export_text(dtcs[0])\n",
        "print(text_representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsuFUM2nlP5"
      },
      "source": [
        "for i, dtc in enumerate(dtcs):\n",
        "  viz = dtreeviz(dtc, X, y)\n",
        "  viz.scale=1.2\n",
        "  display(viz)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHZ-hHGuY5aG"
      },
      "source": [
        "## 2. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETTKyFmwTae"
      },
      "source": [
        "The `sklearn.ensemble` provides several ensemble algorithms. RandomForest is an averaging algorithm based on randomized decision trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\n",
        "\n",
        "Individual decision trees typically exhibit high variance and tend to overfit.\n",
        "In random forests:\n",
        "* each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
        "* when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset.\n",
        "\n",
        "The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant, hence yielding an overall better model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4hfPUqSZCbH"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for n_est in (1, 4, 50):\n",
        "    rfc = ensemble.RandomForestClassifier(max_depth=4, n_estimators=n_est,)\n",
        "    rfc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (n_est=%d)\" % (rfc.score(X, y), n_est))\n",
        "\n",
        "    # create a mesh to plot in\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    Z = rfc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.contourf(xx, yy, Z, cmap='Pastel1', alpha=0.15)\n",
        "    plt.title(\"Decision surface of rfc (%d)\" % n_est)\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "    \n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY6EVEestysr"
      },
      "source": [
        "plt.figure(dpi=300)\n",
        "tree.plot_tree(rfc.estimators_[20]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-YP_GTWsLZI"
      },
      "source": [
        "  viz = dtreeviz(rfc.estimators_[20], X, y)\n",
        "  viz.scale=1.2\n",
        "  display(viz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puQNgKN0wS7H"
      },
      "source": [
        "## 3. Boosted Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkL6_R05wez3"
      },
      "source": [
        "Another approach to the ensemble tree modeling is Boosted Decision Trees. In a boosting framework, the treees are created sequentially. This way each next tree reduces error of the ensamble, by fitting residuals of previous trees.\n",
        "\n",
        "Usually shallow trees are used in boosting framework. In boosting primarily the bias is reduced, thus increasing variance. Interpretability is low.\n",
        "\n",
        "To avoid overfitting the learning rate and subsampling parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qVKJ-YawFo9"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for n_est in (1, 4, 50):\n",
        "    dtc = ensemble.GradientBoostingClassifier(max_depth=1, n_estimators=n_est,\n",
        "                                              learning_rate=0.1, subsample=0.5)\n",
        "    dtc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (n_est=%d)\" % (dtc.score(X, y), n_est))\n",
        "\n",
        "    # create a mesh to plot in\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    Z = dtc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.contourf(xx, yy, Z, cmap='Pastel1', alpha=0.15)\n",
        "    plt.title(f'Decision surface of DTC ({n_est})')\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "    \n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhycm2S6wbz"
      },
      "source": [
        "## EXERCISE 3 : Random forest classifier for FMNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I20uBLiMoURH"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "n = len(train_labels)\n",
        "x_train = train_images.reshape((n, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd59TNBWfHgX"
      },
      "source": [
        "Classify fashion MNIST images with Random Forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8waJCx33peIG"
      },
      "source": [
        "# 1. Create classifier. As the number of features is big (784), use bigger tree\n",
        "# depth (max_depth parameter), try in range 10-500.\n",
        "\n",
        "# 2. What is the maximum number of leaves in tree of depth n?\n",
        "# To reduce variance we should avoid leaves with too litle samples. You could\n",
        "# limit the total number of tree leaves (max_leaf_nodes parameter) to 10-1000.\n",
        "# Alternatively you can use min_samples_split & min_samples_leaf\n",
        "\n",
        "# 3. Try different number of estimators (n_estimators)\n",
        "\n",
        "# 4. Fit the model\n",
        "\n",
        "# 5. Inspect training and test accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dG6U6s3T95t"
      },
      "source": [
        "## EXERCISE 4: Random forest regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlypA1MaT95u"
      },
      "source": [
        "X, y, df = house_prices_dataset(return_df=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYTwkoSpT95u"
      },
      "source": [
        "Predict the house prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4wobzB_T95u"
      },
      "source": [
        "# 1. Create regressor. (ensemble.RandomForestRegressor)\n",
        "# Try different number of estimators (n_estimators)\n",
        "\n",
        "# 2. Fit the model\n",
        "# 3. Inspect training and test accuracy\n",
        "\n",
        "# 4. Try to improve performance by adjusting hyperparameters. \n",
        "# How does it compare to linear model?\n",
        "\n",
        "# 5. Use dtreeviz to visualize a tree from the ensamble"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}