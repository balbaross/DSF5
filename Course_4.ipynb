{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "agJ3gkg0tkTJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWmru-7W__vn"
      },
      "source": [
        "# Data Science Fundamentals 5\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Science IT Support, University Of Bern, 2021\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzEnLAMt3De2"
      },
      "source": [
        "# 0. Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVJn0ilgOS8F"
      },
      "source": [
        "from matplotlib import  pyplot as plt\n",
        "import numpy as np\n",
        "from imageio import imread\n",
        "import pandas as pd\n",
        "from time import time as timer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "from matplotlib import animation\n",
        "from matplotlib import cm\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAAjJuenj1u0"
      },
      "source": [
        "# 1. Neural Networks Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AXSFimKkt91"
      },
      "source": [
        "## 1. Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoFa0c0Xj_6m"
      },
      "source": [
        "(Artificial) Neural network consists of layers of neurons. Artificial neuron, or perceptron, is in fact inspired by a biological neuron.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Perceptron.png\" alt=\"drawing\" width=\"30%\"/>\n",
        "\n",
        "Such neuron first calculates the linear transformation of the input vector $\\bar x$: \n",
        "$$z = \\bar W \\cdot \\bar x + b = \\sum {W_i x_i} + b$$ where $\\bar W$ is vector of weights and $b$ - bias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7cZipYkZlH9"
      },
      "source": [
        "This is effectively linear regression. You can combine those parallely to make a multidimensional prediction, e.g.:\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/2Perceptrons.png\" alt=\"drawing\" width=\"30%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-mH0li3kzNi"
      },
      "source": [
        "## 2. Nonlinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74xUp5qkxCy"
      },
      "source": [
        "Combining multiple such objects performing linear transformation sequentially would not bring any additional benefit, as the combined output would still be a linear combination of the inputs.\n",
        "\n",
        "What gives actual power to neurons, is that they additionally perform the nonlinear transformation of the result using activation function $f$ $$y = f(z)$$\n",
        "\n",
        "The most commonly used non-linear transformations are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUo59ubLc_fi"
      },
      "source": [
        "def ReLU(z):\n",
        "  return np.clip(z, a_min=0, a_max=np.max(z))\n",
        "def SELU(z, a=1):\n",
        "  p = np.clip(z, a_min=0, a_max=np.max(z))\n",
        "  n = np.clip(z, a_min=np.min(z), a_max=0)\n",
        "  return p + (np.exp(n)-1) * a\n",
        "def LReLU(z, a=0.1):\n",
        "  return np.clip(z, a_min=0, a_max=np.max(z)) + np.clip(z, a_min=np.min(z), a_max=0) * a\n",
        "def sigmoid(z):\n",
        "  return 1/(1 + np.exp(-z)) \n",
        "def step(z):\n",
        "  return np.heaviside(z, 0)\n",
        "fig, ax = plt.subplots(1, 6, figsize=(18, 3))\n",
        "z = np.linspace(-10, 10, 100)\n",
        "ax[0].plot(z, ReLU(z))\n",
        "ax[0].set_title('Rectified Linear Unit (LU)')\n",
        "ax[1].plot(z, LReLU(z))\n",
        "ax[1].set_title('Leaky Rectified LU')\n",
        "ax[2].plot(z, SELU(z))\n",
        "ax[2].set_title('Scaled Exponential LU')\n",
        "ax[3].plot(z, sigmoid(z))\n",
        "ax[3].set_title(r'$\\sigma$(z)=$\\frac{1}{1+e^z}$')\n",
        "ax[4].plot(z, np.tanh(z))\n",
        "ax[4].set_title('Hyperbolic tangent');\n",
        "ax[5].plot(z, step(z))\n",
        "ax[5].text(-6, 0.5, 'NOT USED', size=19, c='r')\n",
        "ax[5].set_title('Step function');\n",
        "for axi in ax:\n",
        "  axi.set_xlabel('z')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEbaIujf0DV"
      },
      "source": [
        "And the reason we don't use a simple step function, is that it's everywhere either not differentiable or its derivative is zero.\n",
        "\n",
        "The last nonlinearity to mention here is *softmax*:\n",
        "$$y_i = SoftMax(\\bar z)_i = \\frac{ e^{z_i}}{\\sum_j e^{z_j}}$$\n",
        "\n",
        "While each $z_i$ can have any value, the corresponding $y_i\\in[0,1]$, and $\\sum_i y_i=1$, just like probabilities! \n",
        "\n",
        "While these $y_i$ are only pseudo-probabilities, this nonlinearity allows one to model probabilities, e.g. of a data-point belonging to a certain class.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QZtmo6Vk2rp"
      },
      "source": [
        "## 3. Fully connected net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM5z2czOc_4A"
      },
      "source": [
        "In a fully connected neural network each layer is a set of N neurons, performing different transformations of all the same layer's inputs $\\bar x = [x_i]$ producing output vector $\\bar y = [y_j]_{i=1..N}$: $$y_j = f(\\bar W_j \\cdot \\bar x + b_j)$$\n",
        "\n",
        "Since output of each layer forms input of next layer, one can write for layer $l$ (upper index denotes layer): $$x^l_j = f(\\bar W^l_j \\cdot \\bar x^{l-1} + b^l_j)$$ where $\\bar x^0$ is network's input vector.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u62hCoFbklaW"
      },
      "source": [
        "## 4. Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-58_mV6ElY5C"
      },
      "source": [
        "The last part of the puzzle is the measure of network performance, which is used to optimize the network's parameters $W^l_j$ and $b^l_j$.\n",
        "Denoting the network's output for an input $x_i$ as $\\hat y_i=\\hat y_i(x_i)$ and given the label $y_i$:\n",
        "\n",
        "1. In case of regression loss shows \"distance\" from target values:\n",
        "* L2 (MSE): $L = \\sum_i (y_i-\\hat y_i)^2$\n",
        "* L1 (MAE): $L = \\sum_i |y_i-\\hat y_i|$\n",
        "\n",
        "1. In case of classification we can use cross-entropy, which shows \"distance\" from target distribution:\n",
        "$$L = - \\sum_i \\sum_c y_{i,c} \\log(\\hat y_{i,c})$$\n",
        "Here $\\hat y_{i,c}$ - pseudo-probability of $x_i$ belonging to class $c$ and $y_{i,c}$ uses 1-hot encoding:\n",
        "\n",
        "$$y_{i,c}= \n",
        "\\begin{cases}\n",
        "    1,& \\text{if } x_i \\text{ belongs to class } c\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--9TcJQ8Rahm"
      },
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urF63xIfRah0"
      },
      "source": [
        "Training of neural networks is performed iteratively. The weights  $W^l_j$ and $b^l_j$ are updated on each iteration of training according to the value of the derivative of the loss function with respect to corresponding parameter:\n",
        "$$W^l_j \\rightarrow W^l_j - \\lambda \\frac{\\partial L}{\\partial W^l_j }$$\n",
        "$$b^l_j \\rightarrow b^l_j - \\lambda \\frac{\\partial L}{\\partial b^l_j },$$\n",
        "\n",
        "This is Gradient Descent optimization with learning rate $\\lambda$. The partial derivatives are calculated by the chain law, and this approach is known as [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).\n",
        "\n",
        "In practice often for each iteration the loss $L$ is evaluated not on all samples, but on a sub-sample, so-called *minibatch* (sometimes - just batch). In most cases the sample order and selection for each minibatch is performed at random rendering this approach to be stochastic (thus it's called [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)). One iteration through all training data in minibatches is called *epoch*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrwaonuB4F8m"
      },
      "source": [
        "# 2. Regression with neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUCua55tE85o"
      },
      "source": [
        "Here we will build a neural network to perform regression in 2D: we will fit $(x,y)$ pixel coordinates of an image to the pixel brightness at that location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6rG4pXiwT1o"
      },
      "source": [
        "url = 'https://github.com/neworldemancer/DSF5/raw/master/figures/unibe.jpg'\n",
        "image_big = imread(url)\n",
        "image_big = image_big[...,0:3]/255\n",
        "plt.imshow(image_big)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MzVfGJocmDo"
      },
      "source": [
        "def subsample(im, factor):\n",
        "  h, w = im.shape[:2]\n",
        "  h = int(h/factor)\n",
        "  w = int(w/factor)\n",
        "\n",
        "  img = Image.fromarray((im*255).astype('uint8'))\n",
        "  newimg = img.resize((w, h), Image.ANTIALIAS)\n",
        "  return np.asarray(newimg)/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iGxdgfwEeiD"
      },
      "source": [
        "image = subsample(image_big, 15)\n",
        "image = image.mean(axis=2, keepdims=True)\n",
        "plt.imshow(image[...,0], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hGEsYDX4hwu"
      },
      "source": [
        "h, w, c = image.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RiEARRw4wAP"
      },
      "source": [
        "X0 = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
        "X = np.stack(X0, axis=-1).reshape((-1, 2))\n",
        "\n",
        "Y = image.reshape((-1, c))\n",
        "X.shape, Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBVWYpq75GSa"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(2,)),\n",
        "  tf.keras.layers.Dense(c, activation='sigmoid'),\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mae',\n",
        "              metrics=['mse'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR7OlLN2XEo1"
      },
      "source": [
        "hist = model.fit(X, Y, epochs=500, batch_size=2048)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xUI4w3zXdoy"
      },
      "source": [
        "Y_p = model.predict(X)\n",
        "Y_p = Y_p.reshape((h,w,c))\n",
        "im = plt.imshow(Y_p[...,0], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9yeAZ9zaQH9"
      },
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "axs[0].plot(hist.epoch, hist.history['loss'])\n",
        "axs[0].set_title('loss')\n",
        "axs[1].plot(hist.epoch, hist.history['mse'])\n",
        "axs[1].set_title('mse')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxw90MRNDJu9"
      },
      "source": [
        "What is actually happening here? We fit an image with an $\\mathbb{R}^2 \\rightarrow \\mathbb{R}$ funcion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X27VLMYS8R8D"
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.view_init(elev=90., azim=-90)  # azim=-50, elev=30\n",
        "\n",
        "x = X[:,0].reshape((h,w))\n",
        "y = X[:,1].reshape((h,w))\n",
        "z = Y.reshape((h,w))\n",
        "zp = Y_p.reshape((h,w))\n",
        "\n",
        "ds = 2\n",
        "x = subsample(x, ds)\n",
        "y =-subsample(y, ds)\n",
        "z = subsample(z, ds)\n",
        "zp = subsample(zp, ds)\n",
        "\n",
        "surf = ax.plot_surface(x,y,z, cmap='coolwarm', linewidth=0, antialiased=False, alpha=0.3)\n",
        "#surf = ax.plot_surface(x,y,zp, cmap='coolwarm', linewidth=0, antialiased=False, alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLyx-kYXwQe"
      },
      "source": [
        "Let's try the same with an RGB image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR3Te8_NX43W"
      },
      "source": [
        "image = subsample(image_big, 15)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gubPoLVcbzdB"
      },
      "source": [
        "h, w, c = image.shape\n",
        "X = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
        "X = np.stack(X, axis=-1).reshape((-1, 2))\n",
        "\n",
        "Y = image.reshape((-1, c))\n",
        "X.shape, Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg1iKZ8LXl1R"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(2,)),\n",
        "  tf.keras.layers.Dense(c, activation='sigmoid'),\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mae',\n",
        "              metrics=['mse'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLrIjf0YAUF"
      },
      "source": [
        "But now we will save images during the course of training, at first after 2, then 4, 8, 16, etc epochs.\n",
        "(**Remember**: call to `model.fit` does NOT reinitialize trainable variables. Every time it continues from the previous state):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ezFwya85m4M"
      },
      "source": [
        "ims = []\n",
        "loss_hist = []\n",
        "n_ep_pow = 11\n",
        "save_epochs = [0]+[2**i for i in range(n_ep_pow)]\n",
        "n_ep_tot = save_epochs[-1]\n",
        "\n",
        "ne = 2\n",
        "print(f'To be  trained for total number of epochs:{n_ep_tot*ne}')\n",
        "\n",
        "for i in range(n_ep_tot+1):\n",
        "  if i % 100 == 0:\n",
        "    print(f'epoch {i*ne}', end='\\n')\n",
        "  hist = model.fit(X, Y, epochs=ne, batch_size=1*2048, verbose=0)\n",
        "  loss_hist += hist.history['loss']\n",
        "\n",
        "  if i in save_epochs:\n",
        "    Y_p = model.predict(X)\n",
        "    Y_p = Y_p.reshape((h, w, c))\n",
        "    ims.append(Y_p)\n",
        "\n",
        "plt.plot(loss_hist)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFdyqtEyx3-f"
      },
      "source": [
        "%%capture\n",
        "\n",
        "fig = plt.figure()\n",
        "im = plt.imshow(ims[0])\n",
        "\n",
        "def animate(i):\n",
        "    img = ims[i]\n",
        "    im.set_data(img)\n",
        "    return im\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(ims))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxFYMEy5WmF4"
      },
      "source": [
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wc9tXb9a8_q"
      },
      "source": [
        "While the colors properly represent the target image, our model still poses very limited capacity, allowing it to effectively represent only 3 boundaries.\n",
        "\n",
        "Let's upscale our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHmrapmHbWxN"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(2,)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(c, activation='sigmoid'),\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), # optimizer='adam',\n",
        "              loss='mae',\n",
        "              metrics=['mse'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmAqFs7MfThU"
      },
      "source": [
        "ims = []\n",
        "loss_hist = []\n",
        "n_ep_pow = 13\n",
        "save_epochs = [0]+[2**i for i in range(n_ep_pow)]\n",
        "n_ep_tot = save_epochs[-1]\n",
        "\n",
        "ne = 2\n",
        "print(f'total number of epochs trained:{n_ep_tot*ne}')\n",
        "\n",
        "for i in range(n_ep_tot+1):\n",
        "  if i % 100 == 0:\n",
        "    print(f'epoch {i*ne}', end='\\n')\n",
        "  hist = model.fit(X, Y, epochs=ne, batch_size=1*2048, verbose=0)\n",
        "  loss_hist += hist.history['loss']\n",
        "\n",
        "  if i in save_epochs:\n",
        "    Y_p = model.predict(X)\n",
        "    Y_p = Y_p.reshape((h, w, c))\n",
        "    ims.append(Y_p)\n",
        "\n",
        "plt.plot(loss_hist)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GruTrQNeC4A"
      },
      "source": [
        "%%capture\n",
        "fig = plt.figure()\n",
        "im = plt.imshow(ims[0])\n",
        "\n",
        "def animate(i):\n",
        "    img = ims[i]\n",
        "    im.set_data(img)\n",
        "    return im\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(ims))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCAOLpsMeNRA"
      },
      "source": [
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEVkc53u3Vz_"
      },
      "source": [
        "We can also now render the fitted image at any resolution, by evaluating values in between of the original pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV0q97Ip3dik"
      },
      "source": [
        "scale = 30\n",
        "w_rend = w * scale\n",
        "h_rend = h * scale\n",
        "X_rend = np.meshgrid(np.linspace(0, 1, w_rend), np.linspace(0, 1, h_rend))\n",
        "X_rend = np.stack(X_rend, axis=-1).reshape((-1, 2))\n",
        "\n",
        "Y_rend = model.predict(X_rend)\n",
        "Y_rend = Y_rend.reshape((h_rend, w_rend, c))\n",
        "\n",
        "plt.imshow(Y_rend)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt1q-Nez5Hak"
      },
      "source": [
        "plt.imshow(image_big)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCFB8rQ15Tf9"
      },
      "source": [
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOk-BLOFeV61"
      },
      "source": [
        "## EXERCISE 4. Regression on an image with Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09yEvb4aebd5"
      },
      "source": [
        "Load some image, downscale to a similar resolution, and train a deeper model, for example 5 layers, more parameters in widest layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K415Bvdguld7"
      },
      "source": [
        "# 1. Load your image\n",
        "\n",
        "# 2. build a deeper model\n",
        "\n",
        "# 3. inspect the evolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCQ5wxQCgVXD"
      },
      "source": [
        "# 3. Classification of the F-MNIST dataset with neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSRc4lzeYgxX"
      },
      "source": [
        "## 0. Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM_660aCYmnS"
      },
      "source": [
        "We will create a model for classification of the F-MNIST dataset that we go acquainted with in previous sessions. We will normalize the inputs to have values $\\in[0,1]$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maLerZKejJxG"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agJ3gkg0tkTJ"
      },
      "source": [
        "## 1. Building a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0dtcC9MZG3N"
      },
      "source": [
        "print(x_train[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjVnBkWjZHVf"
      },
      "source": [
        "The size of each image sample $-\\; 28\\times28\\text{ pixels}\\;-\\;$ defines the input size for our neural network. Network's output - probabilities of belonging to each of the 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPs6sljtjDUE"
      },
      "source": [
        "The following creates a 'model'. It is an object containing the neural network model itself - a simple 3-layer fully connected neural network, optimization parameters, as well as the interface for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "LC1SpGLbtkTL"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFwr5MLMjxI1"
      },
      "source": [
        "Model summary provides information about the model's layers and trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCttp5zeb5l2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P18eyAQHqZGG"
      },
      "source": [
        "## 2. Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNIdb5Gtlr32"
      },
      "source": [
        "The `fit` function is the interface for model training. \n",
        "Here one can specify training and validation datasets, minibatch size, and the number of training epochs.\n",
        "\n",
        "Here during training we also save the trained models checkpoints after each epoch of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-OxT0aNVx-y"
      },
      "source": [
        "save_path = 'save/mnist_{epoch}.ckpt'\n",
        "save_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, save_weights_only=True)\n",
        "\n",
        "hist = model.fit(x=x_train, y=y_train,\n",
        "                 epochs=50, batch_size=128, \n",
        "                 validation_data=(x_test, y_test),\n",
        "                 callbacks=[save_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l9Gz1e4V-7Q"
      },
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "axs[0].plot(hist.epoch, hist.history['loss'])\n",
        "axs[0].plot(hist.epoch, hist.history['val_loss'])\n",
        "axs[0].legend(('training loss', 'validation loss'), loc='lower right')\n",
        "axs[1].plot(hist.epoch, hist.history['accuracy'])\n",
        "axs[1].plot(hist.epoch, hist.history['val_accuracy'])\n",
        "\n",
        "axs[1].legend(('training accuracy', 'validation accuracy'), loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUnlYrfaBmQ8"
      },
      "source": [
        "Current model performance can be evaluated, e.g. on the test dataset:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cq_gqG4V9il"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-5qgb0rDyj4"
      },
      "source": [
        "We can test trained model on an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPU8mOg2DVSO"
      },
      "source": [
        "im_id = 0\n",
        "y_pred = model(x_test)\n",
        "\n",
        "y_pred_most_probable = np.argmax(y_pred[im_id])\n",
        "print('true lablel: ', y_test[im_id],\n",
        "      '; predicted: ',  y_pred_most_probable,\n",
        "      f'({class_names[y_pred_most_probable]})')\n",
        "plt.imshow(x_test[im_id], cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uPb0WPTk6oq"
      },
      "source": [
        "As well as inspect on which samples does the model fail:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKx-umE7R6AL"
      },
      "source": [
        "y_pred_most_probable_all = np.argmax(y_pred, axis=1)\n",
        "wrong_pred_map = y_pred_most_probable_all!=y_test\n",
        "wrong_pred_idx = np.arange(len(wrong_pred_map))[wrong_pred_map]\n",
        "\n",
        "im_id = wrong_pred_idx[0]\n",
        "\n",
        "y_pred_most_probable = y_pred_most_probable_all[im_id]\n",
        "print('true lablel: ', y_test[im_id],\n",
        "      f'({class_names[y_test[im_id]]})',\n",
        "      '; predicted: ',  y_pred_most_probable,\n",
        "      f'({class_names[y_pred_most_probable]})')\n",
        "plt.imshow(x_test[im_id], cmap='gray');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkrIXxYmqiyR"
      },
      "source": [
        "## 3. Loading trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt_BelVEdH1y"
      },
      "source": [
        "model.load_weights('save/mnist_1.ckpt')\n",
        "model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "model.load_weights('save/mnist_12.ckpt')\n",
        "model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "model.load_weights('save/mnist_18.ckpt')\n",
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxIIoNRYqqd_"
      },
      "source": [
        "## 4. Inspecting trained variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5waklWBUBwuO"
      },
      "source": [
        "We can obtain the trained variables from model layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-nnCph8rU01"
      },
      "source": [
        "l = model.get_layer(index=1)\n",
        "w, b = l.weights\n",
        "\n",
        "w = w.numpy()\n",
        "b = b.numpy()\n",
        "print(w.shape, b.shape)\n",
        "w = w.reshape((28,28,-1)).transpose((2, 0, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfmd5YeUCCKO"
      },
      "source": [
        "Let's visualize first 10:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8UOgBWJfzMg"
      },
      "source": [
        "n = 10\n",
        "fig, axs = plt.subplots(1, n, figsize=(4.1*n,4))\n",
        "for i, wi in enumerate(w[:n]):\n",
        "  axs[i].imshow(wi, cmap='gray')\n",
        "  axs[i].set_title(class_names[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i02IbJ2gtkTT"
      },
      "source": [
        "## 6. Inspecting gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uuktPZ9CX8W"
      },
      "source": [
        "We can also evaluate the gradients of each output with respect to an input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L371n9COtkTU"
      },
      "source": [
        "idx = 112\n",
        "inp_v = x_train[idx:idx+1]  # use some image to compute gradients with respect to\n",
        "\n",
        "inp = tf.constant(inp_v)  # create tf constant tensor\n",
        "with tf.GradientTape() as tape:  # gradient tape for gradint evaluation\n",
        "  tape.watch(inp)  # take inp as variable\n",
        "  preds = model(inp) # evaluate model output\n",
        "\n",
        "grads = tape.jacobian(preds, inp)  # evaluate d preds[i] / d inp[j]\n",
        "print(grads.shape, '<- (Batch_preds, preds[i], Batch_inp, inp[y], inp[x])')\n",
        "grads = grads.numpy()[0,:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83mY_1BKiIIB"
      },
      "source": [
        "print('prediction:', np.argmax(preds[0]))\n",
        "fig, axs = plt.subplots(1, 11, figsize=(4.1*11,4))\n",
        "axs[0].imshow(inp_v[0])\n",
        "axs[0].set_title('raw')\n",
        "vmin,vmax = grads.min(), grads.max()\n",
        "for i, g in enumerate(grads):\n",
        "  axs[i+1].imshow(g, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "  axs[i+1].set_title(r'$\\frac{\\partial\\;P(digit\\,%d)}{\\partial\\;input}$' % i, fontdict={'size':16})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDS71kLLmRE2"
      },
      "source": [
        "## EXERCISE 1: Train deeper network for F-MNIST classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eyO5lzOmWAG"
      },
      "source": [
        "Make a deeper model, with wider layers. Remember to use the `'softmax'` activation in the last layer, as required for the classification task to encode pseudoprobabilities. In the other layers you could use `'relu'`.\n",
        "\n",
        "Try to achieve 90% accuracy.\n",
        "Does your model overfit?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CUw7uHUxSoQ"
      },
      "source": [
        "# 1. create model\n",
        "# 2. train the model\n",
        "# 3. plot the loss and accuracy evolution during training\n",
        "# 4. evaluate model in best point (before overfitting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWm4FgSu-tK0"
      },
      "source": [
        "# 4. Extras and Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OReFhRTHxCdl"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/cheatsheet.png\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_65g5ZdTxCdm"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/clusters.png\" width=\"100%\"/>"
      ]
    }
  ]
}